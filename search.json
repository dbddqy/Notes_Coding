[{"title":"[Exercise] Linear Control","url":"https://dbddqy.github.io/2020/08/12/e-linear_control/","content":"<h1 id=\"Linear-Control\"><a href=\"#Linear-Control\" class=\"headerlink\" title=\"Linear Control\"></a>Linear Control</h1><p><a href=\"https://github.com/dbddqy/Note/tree/master/Intro_to_Robotics/exercise_linear_control\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/Github-Source%20Code-blue\" alt=\"\"></a></p>\n<h2 id=\"Second-order-Linear-Systems\"><a href=\"#Second-order-Linear-Systems\" class=\"headerlink\" title=\"Second-order Linear Systems\"></a>Second-order Linear Systems</h2><p>The simulation of such system according to book 9.3</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">System</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, x0, v0, m=<span class=\"number\">1.</span>, b=<span class=\"number\">2.</span>, k=<span class=\"number\">1.</span>, control=False)</span>:</span></span><br><span class=\"line\">        self.m = m</span><br><span class=\"line\">        self.b = b</span><br><span class=\"line\">        self.k = k</span><br><span class=\"line\">        self.x = x0</span><br><span class=\"line\">        self.v = v0</span><br><span class=\"line\">        self.control = control</span><br><span class=\"line\">        self.a = (self.f() - self.b*self.v - self.k*self.x) / self.m</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">f</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.control:</span><br><span class=\"line\">            kp = <span class=\"number\">16.</span></span><br><span class=\"line\">            kv = <span class=\"number\">2</span>*np.sqrt(kp)</span><br><span class=\"line\">            f_prime = -kp*self.x-kv*self.v</span><br><span class=\"line\">            f = f_prime*self.m + self.b*self.v+self.k*self.x</span><br><span class=\"line\">            <span class=\"keyword\">return</span> f</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">0.</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">update</span><span class=\"params\">(self, dt=<span class=\"number\">0.1</span>)</span>:</span></span><br><span class=\"line\">        self.v += (self.a * dt)</span><br><span class=\"line\">        self.x += (self.v * dt)</span><br><span class=\"line\">        self.a = (self.f() - self.b*self.v - self.k*self.x) / self.m + np.random.normal(<span class=\"number\">0.0</span>, <span class=\"number\">0.00</span>)</span><br></pre></td></tr></table></figure>\n\n<p>If no active control is applied, and $b^2&gt;4mk$, the response is <strong>overdamped</strong>.</p>\n<p>Result (b =4, m = 1.0, k=1.0)</p>\n<p><img src=\"https://github.com/dbddqy/Note/raw/master/Intro_to_Robotics/exercise_linear_control/b=4_no_control.png\" alt=\"\"></p>\n<p>If $b^2=4mk$, the response is <strong>critically damped</strong>.</p>\n<p>Result (b =2, m = 1.0, k=1.0)</p>\n<p><img src=\"https://github.com/dbddqy/Note/raw/master/Intro_to_Robotics/exercise_linear_control/b=2_no_control.png\" alt=\"\"></p>\n<p>If $b^2&lt;4mk$, the response is <strong>underdamped</strong>.</p>\n<p>Result (b =0.5, m = 1.0, k=1.0)</p>\n<p><img src=\"https://github.com/dbddqy/Note/raw/master/Intro_to_Robotics/exercise_linear_control/b=0.5_no_control.png\" alt=\"\"></p>\n<p>According to book 9.5, apply active control to make the system <strong>critically damped</strong>.</p>\n<p>Result (kp=4)</p>\n<p><img src=\"https://github.com/dbddqy/Note/raw/master/Intro_to_Robotics/exercise_linear_control/kp=4_control.png\" alt=\"\"></p>\n<p>Result (kp=16)</p>\n<p><img src=\"https://github.com/dbddqy/Note/raw/master/Intro_to_Robotics/exercise_linear_control/kp=16_control.png\" alt=\"\"></p>\n<h2 id=\"Trajectory-control\"><a href=\"#Trajectory-control\" class=\"headerlink\" title=\"Trajectory control\"></a>Trajectory control</h2><p>The force function changed to:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">f</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> self.control:</span><br><span class=\"line\">        kp = <span class=\"number\">16.</span></span><br><span class=\"line\">        kv = <span class=\"number\">2</span>*np.sqrt(kp)</span><br><span class=\"line\">        f_prime = kp*(self.goal[<span class=\"number\">0</span>]-self.x)+kv*(self.goal[<span class=\"number\">1</span>]-self.v)+self.goal[<span class=\"number\">2</span>]</span><br><span class=\"line\">        f = f_prime*self.m + self.b*self.v+self.k*self.x</span><br><span class=\"line\">        <span class=\"keyword\">return</span> f</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0.</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Step-Function\"><a href=\"#Step-Function\" class=\"headerlink\" title=\"Step Function\"></a>Step Function</h3><p>Trajectory is set as:</p>\n<ul>\n<li>start: x = 2, v = 0, a = 0</li>\n<li>0~3s: x = 0, v = 0, a = 0</li>\n<li>3~7s: x = 4, v = 0, a = 0</li>\n<li>7~10s: x = 2, v = 0, a = 0</li>\n</ul>\n<p>Result (kp=4)</p>\n<p><img src=\"https://github.com/dbddqy/Note/raw/master/Intro_to_Robotics/exercise_linear_control/tra_kp=4.png\" alt=\"\"></p>\n<p>Result (kp=16)</p>\n<p><img src=\"https://github.com/dbddqy/Note/raw/master/Intro_to_Robotics/exercise_linear_control/tra_kp=16.png\" alt=\"\"></p>\n<p>So, higher kp, faster the response, “stiffer” the system</p>\n<h3 id=\"Cubic-Polynomial\"><a href=\"#Cubic-Polynomial\" class=\"headerlink\" title=\"Cubic Polynomial\"></a>Cubic Polynomial</h3><ul>\n<li>start: x = 2, v = 0, a = 0</li>\n<li>trajectory follows polynomial: $y = 0.05(x^3-15x^2+63x)$</li>\n</ul>\n<p>Result (kp=16)</p>\n<p><img src=\"https://github.com/dbddqy/Note/raw/master/Intro_to_Robotics/exercise_linear_control/cubic_polynomial.png\" alt=\"\"></p>\n","categories":["exercise"],"tags":["Intro to Robotics, Mechanics and Control"]},{"title":"[Exercise] Kalman Filter","url":"https://dbddqy.github.io/2020/08/06/e-kalman_filter/","content":"<h1 id=\"Kalman-Filter\"><a href=\"#Kalman-Filter\" class=\"headerlink\" title=\"Kalman Filter\"></a>Kalman Filter</h1><p><a href=\"https://github.com/dbddqy/Note/blob/master/Probabilistic_Robotics/exercise_kf/kf.py\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/Github-Source%20Code-blue\" alt=\"\"></a></p>\n<p>Ground truth: y = 1.0, generate noisy data.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generate_data</span><span class=\"params\">(num=<span class=\"number\">100</span>)</span>:</span></span><br><span class=\"line\">    x0 = np.arange(<span class=\"number\">0.</span>, <span class=\"number\">1.</span>, <span class=\"number\">1.</span>/num)</span><br><span class=\"line\">    y_true = np.ones([num, ])</span><br><span class=\"line\">    y0 = y_true + np.random.normal(<span class=\"number\">0.</span>, <span class=\"number\">0.1</span>, num)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> x0, y0, y_true</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://github.com/dbddqy/Note/raw/master/Probabilistic_Robotics/exercise_kf/pics/data.png\" alt=\"\"></p>\n<p>Assume we have the prediction model y = x (which means the signal is constant), and observation model is also set to y = x</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">F = <span class=\"number\">1.</span></span><br><span class=\"line\">H = <span class=\"number\">1.</span></span><br></pre></td></tr></table></figure>\n\n<p>Set variance and initial value:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># prediction variance: Q, observation variance: R</span></span><br><span class=\"line\">Q = <span class=\"number\">0.1</span></span><br><span class=\"line\">R = <span class=\"number\">0.01</span></span><br><span class=\"line\"><span class=\"comment\"># initial value</span></span><br><span class=\"line\">mu_0 = <span class=\"number\">1.</span></span><br><span class=\"line\">sigma_0 = <span class=\"number\">1.</span>  <span class=\"comment\"># very uncertain about the initial value</span></span><br></pre></td></tr></table></figure>\n\n<p>Applying the filter:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(NUM):</span><br><span class=\"line\">    <span class=\"comment\"># update prediction</span></span><br><span class=\"line\">    mu_minus = F * mu_plus</span><br><span class=\"line\">    sigma_minus = F * F * sigma_plus + Q</span><br><span class=\"line\">    <span class=\"comment\"># update observation</span></span><br><span class=\"line\">    k = (H * sigma_minus) / (H * H * sigma_minus + R)</span><br><span class=\"line\">    mu_plus = mu_minus + k * (y0[i] - H * mu_minus)</span><br><span class=\"line\">    sigma_plus = (<span class=\"number\">1.</span> - k * H) * sigma_minus</span><br><span class=\"line\">    <span class=\"comment\"># append data</span></span><br><span class=\"line\">    mu_filtered.append(mu_plus)</span><br><span class=\"line\">    sigma_filtered.append(sigma_plus)</span><br></pre></td></tr></table></figure>\n\n<p>Here we tend to believe our observation, so the result:</p>\n<p><img src=\"https://github.com/dbddqy/Note/raw/master/Probabilistic_Robotics/exercise_kf/pics/result_00.png\" alt=\"\"></p>\n<p>If we tend to believe our model,</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># prediction variance: Q, observation variance: R</span></span><br><span class=\"line\">Q = <span class=\"number\">0.0001</span></span><br><span class=\"line\">R = <span class=\"number\">0.1</span></span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://github.com/dbddqy/Note/raw/master/Probabilistic_Robotics/exercise_kf/pics/result_01.png\" alt=\"\"></p>\n<p>KF is not sensitive to the initial value, we can set it very wrong.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># initial value</span></span><br><span class=\"line\">mu_0 = <span class=\"number\">10.</span></span><br><span class=\"line\">sigma_0 = <span class=\"number\">1.</span></span><br></pre></td></tr></table></figure>\n\n<p>And it will be corrected very soon.</p>\n<p><img src=\"https://github.com/dbddqy/Note/raw/master/Probabilistic_Robotics/exercise_kf/pics/result_02.png\" alt=\"\"></p>\n<p>Finally, if the actual model is a bit more complicated, and our prediction model doesn’t match it at all:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># truth y = x ** 2</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generate_data</span><span class=\"params\">(num=<span class=\"number\">100</span>)</span>:</span></span><br><span class=\"line\">    x0 = np.arange(<span class=\"number\">0.</span>, <span class=\"number\">1.</span>, <span class=\"number\">1.</span>/num)</span><br><span class=\"line\">    y_true = x0 * x0</span><br><span class=\"line\">    y0 = y_true + np.random.normal(<span class=\"number\">0.</span>, <span class=\"number\">0.1</span>, num)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> x0, y0, y_true</span><br></pre></td></tr></table></figure>\n\n<p>KF can still improve the result a bit. </p>\n<p><img src=\"https://github.com/dbddqy/Note/raw/master/Probabilistic_Robotics/exercise_kf/pics/result_03.png\" alt=\"\"></p>\n","categories":["exercise"],"tags":["probabilistic robotics"]},{"title":"[Publication] Working with uncertainties, an adaptive fabrication workflow for bamboo structures","url":"https://dbddqy.github.io/2020/08/02/pub-cdrf2020/","content":"<h1 id=\"Working-with-uncertainties-an-adaptive-fabrication-workflow-for-bamboo-structures\"><a href=\"#Working-with-uncertainties-an-adaptive-fabrication-workflow-for-bamboo-structures\" class=\"headerlink\" title=\"Working with uncertainties, an adaptive fabrication workflow for bamboo structures\"></a>Working with uncertainties, an adaptive fabrication workflow for bamboo structures</h1><p><strong>(A collaboration with Ruqing Zhong, Benjamin Kaiser, Long Nguyen, Hans Jakob Wagner, Alexander Verl and Achim Menges)</strong></p>\n<p>This research in presented on the <a href=\"https://www.digitalfutures.world/cdrf2020\" target=\"_blank\" rel=\"noopener\">CERF 2020 Conference</a> and will be soon published by Spinger.</p>\n","categories":["publication"],"tags":[]},{"title":"[Theory] [Probabilistic Robotics] ch2","url":"https://dbddqy.github.io/2020/07/23/t-probabilistic_robotics_ch2/","content":"<h1 id=\"Recursive-State-Estimation\"><a href=\"#Recursive-State-Estimation\" class=\"headerlink\" title=\"Recursive State Estimation\"></a>Recursive State Estimation</h1><h2 id=\"Conditional-Independence\"><a href=\"#Conditional-Independence\" class=\"headerlink\" title=\"Conditional Independence\"></a>Conditional Independence</h2><p><em>x</em> and <em>y</em> are independent under the condition <em>z</em>:<br>$$<br>p(x,y|z) = p(x|z)p(y|z) \\tag{2.17}<br>$$</p>\n<p>which is equivalent to:<br>$$<br>\\begin{cases}<br>     p(x|z) = p(x|z,y)<br>\\\\ p(y|z) = p(y|z,x) \\tag{2.18-19}<br>\\end{cases}<br>$$<br><strong>Proof</strong>:</p>\n<p>WLOG, insert (2.18) into (2.17) right side, get<br>$$<br>p(x|z,y)p(y|z) = \\frac{p(x|z,y)p(y,z)}{p(z)} = \\frac{p(x,y,z)}{p(z)} = p(x,y|z)<br>$$<br>however x and y are not necessarily independent, see book (2.20-21)</p>\n<h2 id=\"Probabilistic-Generative-Laws\"><a href=\"#Probabilistic-Generative-Laws\" class=\"headerlink\" title=\"Probabilistic Generative Laws\"></a>Probabilistic Generative Laws</h2><p><strong>Robot States</strong>: x<sub>0</sub>, x<sub>1</sub>, … , x<sub>t</sub></p>\n<p><strong>Environment Measurement</strong>: z<sub>1</sub>, … , z<sub>t</sub></p>\n<p><strong>Control data</strong>: u<sub>1</sub>, … , u<sub>t</sub></p>\n<p>(assume take action u first, than take measurement z)</p>\n<p>If x is <strong>complete</strong>, then x<sub>t</sub> only depends on x<sub>t-1</sub> and u<sub>t</sub>:<br>$$<br>p(x_t|x_{0:t-1},z_{1:t-1},u_{1:t}) = p(x_t|x_{t-1}, u_t) \\tag{2.31}<br>$$<br>and z<sub>t</sub> depends only on x<sub>t</sub>:<br>$$<br>p(z_t|x_{0:t},z_{1:t-1},u_{1:t}) = p(z_t|x_t) \\tag{2.32}<br>$$<br>(2.31) is called <strong>state transition probability</strong> and (2.32) is called <strong>measurement probability</strong>. They together form the <strong>Hidden Markov Model</strong>.</p>\n<p><img src=\"https://github.com/dbddqy/Note/blob/master/Probabilistic_Robotics/pics/HMM.png?raw=true\" alt=\"\"></p>\n<h2 id=\"Belief-Distributions\"><a href=\"#Belief-Distributions\" class=\"headerlink\" title=\"Belief Distributions\"></a>Belief Distributions</h2><p><strong>Belief</strong>: with all the measurements and control data, the distribution of state <em>x</em>, where the robot believes it is.</p>\n<p>Before measurement:<br>$$<br>\\overline{bel}(x_t) = p(x_t | z_{1:t-1}, u_{1:t}) \\tag{2.34}<br>$$<br>After measurement:<br>$$<br>bel(x_t) = p(x_t | z_{1:t}, u_{1:t}) \\tag{2.33}<br>$$</p>\n<h2 id=\"Bayes-Filters\"><a href=\"#Bayes-Filters\" class=\"headerlink\" title=\"Bayes Filters\"></a>Bayes Filters</h2><p><strong>Prediction</strong>:<br>$$<br>\\overline{bel}(x_i) = \\int{p(x_t | u_t, x_{t-1})bel(x_{t-1})}dx_{t-1}<br>$$<br><strong>Correction</strong> (measurement update):<br>$$<br>bel(x_i) = \\eta{p(z_t | x_t)\\overline{bel}(x_t)}<br>$$</p>\n<h3 id=\"Bayes-Filter-Derivation\"><a href=\"#Bayes-Filter-Derivation\" class=\"headerlink\" title=\"Bayes Filter Derivation\"></a>Bayes Filter Derivation</h3><p><img src=\"https://github.com/dbddqy/Note/raw/master/Probabilistic_Robotics/pics/bayes_filter_0.jpg\" alt=\"\"></p>\n","categories":["theory"],"tags":["probabilistic robotics","robotics"]},{"title":"[Project] [Software] Visual Kinematics","url":"https://dbddqy.github.io/2020/07/17/p-visual_kinematics/","content":"<h1 id=\"Visual-Kinematics\"><a href=\"#Visual-Kinematics\" class=\"headerlink\" title=\"Visual Kinematics\"></a>Visual Kinematics</h1><p><a href=\"https://github.com/dbddqy/visual_kinematics\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/Github-Source%20Code-blue\" alt=\"\"></a> <a href=\"https://pypi.org/project/visual-kinematics/\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/PyPI-0.0.7-orange\" alt=\"\"></a></p>\n<p>This project concentrates on an python-based robotics toolbox for <strong>kinematics calculation</strong>, <strong>robot pose visualization</strong> and <strong>robot path planning</strong>.</p>\n<h2 id=\"Requirements\"><a href=\"#Requirements\" class=\"headerlink\" title=\"Requirements\"></a>Requirements</h2><ul>\n<li>easy to use, only need DH parameters as input (so numerical inverse required).</li>\n<li>support both classic and modified DH parameters</li>\n<li>support robot with different degrees of freedom</li>\n<li>support different representations for rotaion (euler angles, angle axis, quaternion, etc.)</li>\n<li>support high performance analytical inverse (has to be user defined)</li>\n<li>visualizing robot pose and trajectory (a slider for simulating the trajectory)</li>\n<li>support path interpolation in joint space and cartesian space (p2p and linear) </li>\n</ul>\n<h2 id=\"Some-Results\"><a href=\"#Some-Results\" class=\"headerlink\" title=\"Some Results\"></a>Some Results</h2><p><strong>Trajectory:</strong></p>\n<p><img src=\"https://github.com/dbddqy/visual_kinematics/raw/master/pics/trajectory.gif?raw=true\" alt=\"\"></p>\n<p><strong>Linear motion with analytical inverse:</strong></p>\n<p><img src=\"https://github.com/dbddqy/visual_kinematics/raw/master/pics/analytical_inv.gif?raw=true\" alt=\"\"></p>\n<p><strong>7-axis:</strong></p>\n<p><img src=\"https://github.com/dbddqy/visual_kinematics/raw/master/pics/7-axis.gif?raw=true\" alt=\"\"></p>\n","categories":["project"],"tags":["robotics"]},{"title":"[Project] Platform-based Robotic Timber Architecture","url":"https://dbddqy.github.io/2020/06/25/p-digital_future_vision/","content":"<h1 id=\"Platform-based-Robotic-Timber-Architecture\"><a href=\"#Platform-based-Robotic-Timber-Architecture\" class=\"headerlink\" title=\"Platform-based Robotic Timber Architecture\"></a>Platform-based Robotic Timber Architecture</h1><p><strong>This is a Digital Future 2020 workshop given by CAUP (College of Architecture and Urban Planning), Tongji University and ICD (Institute for Computational Design and Construction) University of Stuttgart collaboratively. I was involved as a student research assistant from ICD side and developed the fiducial marker based platform localization strategy.</strong></p>\n<p>Check the project info <a href=\"https://www.digitalfutures.world/workshops-asia-pacific-blog/yuan-menges\" target=\"_blank\" rel=\"noopener\">here</a></p>\n","categories":["project"],"tags":["robotics"]},{"title":"[Project] Robotic Craftsmanship","url":"https://dbddqy.github.io/2020/02/06/p-robotic_craftsmanship/","content":"<h1 id=\"Robotic-Craftsmanship\"><a href=\"#Robotic-Craftsmanship\" class=\"headerlink\" title=\"Robotic Craftsmanship\"></a>Robotic Craftsmanship</h1><p><strong>(A collaboration with Yanan Guo and Ruqing Zhong)</strong></p>\n<p><a href=\"https://github.com/dbddqy/machine_perception/tree/master/demo2.2_detect_cylinder\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/Github-Repo-blue\" alt=\"\"></a></p>\n<p><img src=\"/pics/p-robotic_craftsmanship/result.png\" alt=\"\"></p>\n<p>This is a final project of 2020 behavior fabrication seminar at University of Stuttgart.</p>\n<p>Craftman can intuitively handle material without knowing its precise dimensions in advance. However, robots are still in most of the cases blind and repeating the same motion over and over again.</p>\n<p><img src=\"/pics/p-robotic_craftsmanship/material_uncertainty.png\" alt=\"\"></p>\n<p>This project explores the posibility of using robotic arm to cut the mortise of bamboo connections. As the exact geometry of the bamboo is unknown, the robot should be equipped with some sensor to perceive the material.</p>\n<p><strong>Sensor:</strong></p>\n<ul>\n<li>RGBD camera: Realsense D415</li>\n</ul>\n<p><strong>Algorithms used:</strong></p>\n<ul>\n<li>region growing for segmentation (initial point given by aruco markers)<br>(that is a short cut we take, a more proper and general method can be deep neural network based instance segmentaion, e.g., Mask R-CNN)</li>\n<li>ICP (iterative closest point) for aligning mutiple frames</li>\n<li>least squares cylinder fitting for estimating cylinder parameters</li>\n</ul>\n<p><strong>Segmentation:</strong></p>\n<p><img src=\"/pics/p-robotic_craftsmanship/segmentation.gif\" alt=\"\"></p>\n<p><strong>Robotic milling:</strong></p>\n<p><img src=\"/pics/p-robotic_craftsmanship/milling.gif\" alt=\"\"></p>\n<p><strong>Result:</strong></p>\n<p><img src=\"/pics/p-robotic_craftsmanship/result.gif\" alt=\"\"></p>\n","categories":["project"],"tags":["ITECH master"]},{"title":"[Exercise] Multi-class SVM for solving MNIST","url":"https://dbddqy.github.io/2019/11/20/e-svm_mnist/","content":"<h1 id=\"Multi-class-SVM-for-solving-MNIST\"><a href=\"#Multi-class-SVM-for-solving-MNIST\" class=\"headerlink\" title=\"Multi-class SVM for solving MNIST\"></a>Multi-class SVM for solving MNIST</h1><p><a href=\"https://github.com/dbddqy/Note/tree/master/Detection_Pattern_Recognition/exercise_SVM_MNIST\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/Github-Source%20Code-blue\" alt=\"\"></a></p>\n<p>Use one against the rest strategy, so 10 SVMs are trained. They are stored in class TenClassSVM:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TenClassSVM</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, _alphas, _xs, _ys, _ws, _gamma, _kernel, _C)</span>:</span></span><br><span class=\"line\">        self.gamma = _gamma</span><br><span class=\"line\">        self.kernel = _kernel</span><br><span class=\"line\">        self.C = _C</span><br><span class=\"line\">        self.alphas = _alphas</span><br><span class=\"line\">        self.xs = _xs</span><br><span class=\"line\">        self.ys = _ys</span><br><span class=\"line\">        self.ws = _ws</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">predict</span><span class=\"params\">(self, x, _label)</span>:</span></span><br><span class=\"line\">        alpha_use = self.alphas[_label]</span><br><span class=\"line\">        x_use = self.xs[_label]</span><br><span class=\"line\">        y_use = self.ys[_label]</span><br><span class=\"line\">        pred = self.ws[_label]</span><br><span class=\"line\">        <span class=\"keyword\">for</span> index <span class=\"keyword\">in</span> range(alpha_use.shape[<span class=\"number\">0</span>]):</span><br><span class=\"line\">            pred += (alpha_use[index][<span class=\"number\">0</span>] * y_use[index][<span class=\"number\">0</span>] * self.kernel(x_use[index], x, self.gamma))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> pred</span><br></pre></td></tr></table></figure>\n\n<p>PCA for feature dimension reduction.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># PCA</span></span><br><span class=\"line\">x_train_flattened = np.array(x_train.reshape([N, <span class=\"number\">28</span>*<span class=\"number\">28</span>, <span class=\"number\">1</span>]), dtype=np.float32)</span><br><span class=\"line\">R = np.einsum(<span class=\"string\">\"ijk, ilk -&gt; ijl\"</span>, x_train_flattened, x_train_flattened).mean(axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">e_vals, e_vecs = np.linalg.eig(R)</span><br><span class=\"line\">W = e_vecs[:, :<span class=\"number\">50</span>]</span><br><span class=\"line\">x_train_reduced = W.T.dot(x_train_flattened.reshape([N, <span class=\"number\">28</span>*<span class=\"number\">28</span>]).T).T</span><br></pre></td></tr></table></figure>\n\n<p>The training process of a SVM:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">trainSVM</span><span class=\"params\">(x, y, _kernel, _gamma, _C)</span>:</span></span><br><span class=\"line\">    n = y.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">    P_Mat = get_P_Mat(x, y, kernel=_kernel, _gamma=_gamma)</span><br><span class=\"line\">    <span class=\"comment\"># optimizaton</span></span><br><span class=\"line\">    P = matrix(P_Mat)</span><br><span class=\"line\">    q = matrix(-np.ones([n, <span class=\"number\">1</span>]))</span><br><span class=\"line\">    <span class=\"comment\"># 2*n inequality constraints</span></span><br><span class=\"line\">    G = matrix(np.vstack([-np.eye(n), np.eye(n)]))</span><br><span class=\"line\">    h = matrix(np.vstack([np.zeros([n, <span class=\"number\">1</span>]), _C * np.ones([n, <span class=\"number\">1</span>])]))</span><br><span class=\"line\">    <span class=\"comment\"># 1 equality constraint</span></span><br><span class=\"line\">    A = matrix(y.T)</span><br><span class=\"line\">    b = matrix(<span class=\"number\">0.</span>)</span><br><span class=\"line\">    sol = solvers.qp(P, q, G, h, A, b)</span><br><span class=\"line\">    alpha = np.array(sol[<span class=\"string\">'x'</span>])</span><br><span class=\"line\">    <span class=\"comment\"># get SVs and SOs</span></span><br><span class=\"line\">    SV, SO = [], []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(n):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> alpha[i][<span class=\"number\">0</span>] &lt; <span class=\"number\">0.001</span>:</span><br><span class=\"line\">            <span class=\"keyword\">continue</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> abs(alpha[i][<span class=\"number\">0</span>] - _C) &gt; <span class=\"number\">0.001</span>:</span><br><span class=\"line\">            SV.append(i)</span><br><span class=\"line\">        SO.append(i)</span><br><span class=\"line\">    <span class=\"comment\"># calculate b</span></span><br><span class=\"line\">    w0 = <span class=\"number\">0.</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> SV:</span><br><span class=\"line\">        w0 += y[i][<span class=\"number\">0</span>]</span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> SO:</span><br><span class=\"line\">            w0 -= alpha[j][<span class=\"number\">0</span>] * y[j][<span class=\"number\">0</span>] * P_Mat[j][i]</span><br><span class=\"line\">    w0 /= len(SV)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> alpha[SO], x[SO], y[SO], w0</span><br></pre></td></tr></table></figure>\n\n<p>Save the load models:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># save model</span></span><br><span class=\"line\">model = TenClassSVM(alphas, xs, ys, ws, gamma, kernel, C)</span><br><span class=\"line\">f = open(<span class=\"string\">'model_1.pickle'</span>, <span class=\"string\">'wb'</span>)</span><br><span class=\"line\">pickle.dump(model, f)</span><br><span class=\"line\">f.close()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># load model</span></span><br><span class=\"line\">f = open(<span class=\"string\">'model_1.pickle'</span>, <span class=\"string\">'rb'</span>)</span><br><span class=\"line\">model = pickle.load(f)</span><br><span class=\"line\">f.close()</span><br></pre></td></tr></table></figure>\n\n<p>Make prediction:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">predict</span><span class=\"params\">(x, label, kernel, _gamma)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">global</span> alphas, SOs, ws</span><br><span class=\"line\">    <span class=\"keyword\">global</span> x_train_reduced, y_train</span><br><span class=\"line\">    pred = ws[label]</span><br><span class=\"line\">    <span class=\"keyword\">for</span> index <span class=\"keyword\">in</span> range(alphas[label].shape[<span class=\"number\">0</span>]):</span><br><span class=\"line\">        pred += (alphas[label][index][<span class=\"number\">0</span>] * y_train[SOs[label][index]] * kernel(x_train_reduced[SOs[label][index]], x, _gamma))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> pred</span><br></pre></td></tr></table></figure>\n\n<p>Final result tends to over-fitting (perhaps gamma value has to be further tuned):</p>\n<blockquote>\n<p>training error rate: 0.007500<br>test error rate: 0.100000</p>\n</blockquote>\n","categories":["exercise"],"tags":["detection and pattern recognition"]},{"title":"[Theory] [Detection and Pattern Recognition] ch6","url":"https://dbddqy.github.io/2019/11/16/t-detection_pattern_recognition_ch6/","content":"<h1 id=\"Feature-Dimension-Reduction\"><a href=\"#Feature-Dimension-Reduction\" class=\"headerlink\" title=\"Feature Dimension Reduction\"></a>Feature Dimension Reduction</h1><p>Feature Selection:</p>\n<ul>\n<li>sequential floating forward selection (SFFS)</li>\n</ul>\n<p>Feature Transform:</p>\n<ul>\n<li>principal component analysis (PCA)</li>\n<li>linear discriminant analysis (LDA)</li>\n</ul>\n<h3 id=\"Principal-Component-Analysis\"><a href=\"#Principal-Component-Analysis\" class=\"headerlink\" title=\"Principal Component Analysis\"></a>Principal Component Analysis</h3><img src=\"https://github.com/dbddqy/Note/raw/master/Detection_Pattern_Recognition/pics/PCA.png\" style=\"zoom:80%;\" />\n\n<p>Note:</p>\n<ul>\n<li>powerful tool in data compression</li>\n<li>the optimum transform matrix W depends on signal itself</li>\n</ul>\n","categories":["theory"],"tags":["detection and pattern recognition"]},{"title":"[Exercise] K-means for clustering","url":"https://dbddqy.github.io/2019/11/11/e-k_means/","content":"<h1 id=\"K-means-for-clustering\"><a href=\"#K-means-for-clustering\" class=\"headerlink\" title=\"K-means for clustering\"></a>K-means for clustering</h1><p><a href=\"https://github.com/dbddqy/Note/blob/master/Detection_Pattern_Recognition/exercise_kmeans/k_means.py\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/Github-Source%20Code-blue\" alt=\"\"></a></p>\n<p>Generate 60 data points, and divide them into 4 clusters. Centers of each cluster randomly chosen.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">n = <span class=\"number\">60</span></span><br><span class=\"line\">x_train, y_train, x_test, y_test = generate_data(n)</span><br><span class=\"line\">K = <span class=\"number\">4</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># initialization</span></span><br><span class=\"line\">center_indices, cluster_indices = [], []</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(K):</span><br><span class=\"line\">    center_indices.append(random.randint(<span class=\"number\">0</span>, len(x_train)<span class=\"number\">-1</span>))</span><br><span class=\"line\">    cluster_indices.append([])</span><br></pre></td></tr></table></figure>\n\n<p>Assigning label for all the samples</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(x_train)):</span><br><span class=\"line\">    sample = x_train[i]</span><br><span class=\"line\">    nearest_label = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> label <span class=\"keyword\">in</span> range(K):</span><br><span class=\"line\">        center_index = center_indices[label]</span><br><span class=\"line\">        <span class=\"keyword\">if</span> distance(sample, x_train[center_index]) &lt; distance(sample, x_train[center_indices[nearest_label]]):</span><br><span class=\"line\">            nearest_label = label</span><br><span class=\"line\">    cluster_indices[nearest_label].append(i)</span><br></pre></td></tr></table></figure>\n\n<p>Computing the new center</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">flag = <span class=\"literal\">False</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(K):</span><br><span class=\"line\">    pos_mean = np.mean(x_train[cluster_indices[i]], axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">    nearest_index = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> index <span class=\"keyword\">in</span> range(len(x_train)):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> distance(pos_mean, x_train[index]) &lt; distance(pos_mean, x_train[nearest_index]):</span><br><span class=\"line\">            nearest_index = index</span><br><span class=\"line\">    <span class=\"keyword\">if</span> center_indices[i] != nearest_index:</span><br><span class=\"line\">        flag = <span class=\"literal\">True</span></span><br><span class=\"line\">    center_indices[i] = nearest_index</span><br></pre></td></tr></table></figure>\n\n<p>Result:</p>\n<p><img src=\"https://github.com/dbddqy/Note/raw/master/Detection_Pattern_Recognition/exercise_kmeans/result.png\" alt=\"\"></p>\n","categories":["exercise"],"tags":["detection and pattern recognition"]},{"title":"[Theory] [Detection and Pattern Recognition] ch5","url":"https://dbddqy.github.io/2019/11/07/t-detection_pattern_recognition_ch5/","content":"<h1 id=\"Unsupervised-Learning\"><a href=\"#Unsupervised-Learning\" class=\"headerlink\" title=\"Unsupervised Learning\"></a>Unsupervised Learning</h1><h2 id=\"Clustering\"><a href=\"#Clustering\" class=\"headerlink\" title=\"Clustering\"></a>Clustering</h2><p><strong>K-Means:</strong> </p>\n<img src=\"https://github.com/dbddqy/Note/raw/master/Detection_Pattern_Recognition/pics/k_means.png\" style=\"zoom:80%;\" />\n\n<p>Note:</p>\n<ul>\n<li>multi-class clustering</li>\n<li>sensitive to the choice of the number of clusters c and the initial cluster centers</li>\n</ul>\n<h3 id=\"Other-methods\"><a href=\"#Other-methods\" class=\"headerlink\" title=\"Other methods:\"></a>Other methods:</h3><ul>\n<li>Fuzzy c-means</li>\n<li>mean-shift</li>\n<li>DBSCAN</li>\n</ul>\n","categories":["theory"],"tags":["detection and pattern recognition"]},{"title":"[Exercise] Soft margin SVM","url":"https://dbddqy.github.io/2019/11/06/e-soft_svm/","content":"<h1 id=\"Soft-margin-SVM\"><a href=\"#Soft-margin-SVM\" class=\"headerlink\" title=\"Soft margin SVM\"></a>Soft margin SVM</h1><p><a href=\"https://github.com/dbddqy/Note/blob/master/Detection_Pattern_Recognition/exercise_soft_SVM/soft_margin.py\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/Github-Source%20Code-blue\" alt=\"\"></a></p>\n<p>Data generation:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generate_data</span><span class=\"params\">(n, p=<span class=\"number\">1.0</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># p: percentage of data for training</span></span><br><span class=\"line\">    x1 = np.random.multivariate_normal([<span class=\"number\">4.</span>, <span class=\"number\">3.</span>], [[<span class=\"number\">.4</span>, <span class=\"number\">0.</span>], [<span class=\"number\">0.</span>, <span class=\"number\">.1</span>]], int(<span class=\"number\">1.0</span>*n))  <span class=\"comment\"># simple case</span></span><br><span class=\"line\">    plt.scatter(x1.T[<span class=\"number\">0</span>], x1.T[<span class=\"number\">1</span>], color=<span class=\"string\">\"red\"</span>)</span><br><span class=\"line\">    x2 = np.random.multivariate_normal([<span class=\"number\">6.</span>, <span class=\"number\">0.</span>], [[<span class=\"number\">1.5</span>, <span class=\"number\">.5</span>], [<span class=\"number\">.5</span>, <span class=\"number\">1.5</span>]], int(<span class=\"number\">1.0</span>*n))</span><br><span class=\"line\">    plt.scatter(x2.T[<span class=\"number\">0</span>], x2.T[<span class=\"number\">1</span>], color=<span class=\"string\">\"blue\"</span>)</span><br><span class=\"line\">    <span class=\"comment\"># combine data</span></span><br><span class=\"line\">    x = np.vstack((x1, x2))</span><br><span class=\"line\">    y = np.asarray([[<span class=\"number\">1.</span>]] * n + [[<span class=\"number\">-1.</span>]] * n)</span><br><span class=\"line\">    <span class=\"comment\"># shuffle data</span></span><br><span class=\"line\">    shuffle_idx = np.arange(<span class=\"number\">0</span>, n*<span class=\"number\">2</span>)</span><br><span class=\"line\">    np.random.shuffle(shuffle_idx)</span><br><span class=\"line\">    x_shuffled = x[shuffle_idx]</span><br><span class=\"line\">    y_shuffled = y[shuffle_idx]</span><br><span class=\"line\">    <span class=\"comment\"># split data into training and testing</span></span><br><span class=\"line\">    _x_train = x_shuffled[<span class=\"number\">0</span>:int(n * p)*<span class=\"number\">2</span>]</span><br><span class=\"line\">    _y_train = y_shuffled[<span class=\"number\">0</span>:int(n * p)*<span class=\"number\">2</span>]</span><br><span class=\"line\">    _x_test = x_shuffled[int(n * p)*<span class=\"number\">2</span>:n*<span class=\"number\">2</span>]</span><br><span class=\"line\">    _y_test = y_shuffled[int(n * p)*<span class=\"number\">2</span>:n*<span class=\"number\">2</span>]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> _x_train, _y_train, _x_test, _y_test</span><br></pre></td></tr></table></figure>\n\n<p>RBF kernel:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">kernel_RBF</span><span class=\"params\">(x1, x2, _gamma)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> np.exp(-_gamma * np.sum((x1 - x2) * (x1 - x2)))</span><br></pre></td></tr></table></figure>\n\n<p>Get the matrix for quadratic programming (applying the kernel trick):</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_P_Mat</span><span class=\"params\">(x, y, n, kernel, _gamma)</span>:</span></span><br><span class=\"line\">    _P = np.zeros([n, n], dtype=np.float)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(n):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(n):</span><br><span class=\"line\">            _P[i][j] = y[i][<span class=\"number\">0</span>] * y[j][<span class=\"number\">0</span>] * kernel(x[i].T, x[j].T, _gamma)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> _P</span><br></pre></td></tr></table></figure>\n\n<p>Solve the quadratic programming with cvxopt:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># optimizaton</span></span><br><span class=\"line\">P = matrix(P_Mat)</span><br><span class=\"line\">q = matrix(-np.ones([n*<span class=\"number\">2</span>, <span class=\"number\">1</span>]))</span><br><span class=\"line\"><span class=\"comment\"># 2*n inequality constraints</span></span><br><span class=\"line\">G = matrix(np.vstack([-np.eye(n*<span class=\"number\">2</span>), np.eye(n*<span class=\"number\">2</span>)]))</span><br><span class=\"line\">h = matrix(np.vstack([np.zeros([n*<span class=\"number\">2</span>, <span class=\"number\">1</span>]), C*np.ones([n*<span class=\"number\">2</span>, <span class=\"number\">1</span>])]))</span><br><span class=\"line\"><span class=\"comment\"># 1 equality constraint</span></span><br><span class=\"line\">A = matrix(y_train.T)</span><br><span class=\"line\">b = matrix(<span class=\"number\">0.</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">sol = solvers.qp(P, q, G, h, A, b)</span><br><span class=\"line\">alpha = np.array(sol[<span class=\"string\">'x'</span>])</span><br></pre></td></tr></table></figure>\n\n<p>Select indices of SVs (support vectors) and SOs (support vector and outliers):</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SV, SO = [], []</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(n*<span class=\"number\">2</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> alpha[i][<span class=\"number\">0</span>] &lt; <span class=\"number\">0.001</span>:</span><br><span class=\"line\">        <span class=\"keyword\">continue</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> abs(alpha[i][<span class=\"number\">0</span>]-C) &gt; <span class=\"number\">0.001</span>:</span><br><span class=\"line\">        SV.append(i)</span><br><span class=\"line\">    SO.append(i)</span><br></pre></td></tr></table></figure>\n\n<p>Calculate the offset w0:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">w0 = <span class=\"number\">0.</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> SV:</span><br><span class=\"line\">\tw0 += y_train[i][<span class=\"number\">0</span>]</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> SO:</span><br><span class=\"line\">\t\tw0 -= alpha[j][<span class=\"number\">0</span>] * y_train[j][<span class=\"number\">0</span>] * P_Mat[j][i]</span><br><span class=\"line\">w0 /= len(SV)</span><br></pre></td></tr></table></figure>\n\n<p>Result:</p>\n<p><img src=\"https://github.com/dbddqy/Note/raw/master/Detection_Pattern_Recognition/exercise_soft_SVM/soft_margin.png\" alt=\"\"></p>\n<p>When gamma is set too large, it tends to overfitting.</p>\n<p><img src=\"https://github.com/dbddqy/Note/raw/master/Detection_Pattern_Recognition/exercise_soft_SVM/soft_margin_overfitting_gamma=50.png\" alt=\"\"></p>\n","categories":["exercise"],"tags":["detection and pattern recognition"]},{"title":"[Theory] [Detection and Pattern Recognition] ch4","url":"https://dbddqy.github.io/2019/11/04/t-detection_pattern_recognition_ch4/","content":"<h1 id=\"Supervised-Learning\"><a href=\"#Supervised-Learning\" class=\"headerlink\" title=\"Supervised Learning\"></a>Supervised Learning</h1><h2 id=\"Template-Matching\"><a href=\"#Template-Matching\" class=\"headerlink\" title=\"Template Matching\"></a>Template Matching</h2><h3 id=\"Nearest-Mean\"><a href=\"#Nearest-Mean\" class=\"headerlink\" title=\"Nearest Mean\"></a>Nearest Mean</h3><img src=\"https://github.com/dbddqy/Note/raw/master/Detection_Pattern_Recognition/pics/nearest_mean.png\" style=\"zoom:80%;\" />\n\n<p>Note:</p>\n<ul>\n<li><strong>Mahalanobis Distance</strong> is robust to feature transforms</li>\n<li>multi-class classifier</li>\n<li>fails in <strong>non-linear-separable</strong> dataset.</li>\n</ul>\n<h3 id=\"K-Nearest-Neighbor\"><a href=\"#K-Nearest-Neighbor\" class=\"headerlink\" title=\"K-Nearest Neighbor\"></a>K-Nearest Neighbor</h3><img src=\"https://github.com/dbddqy/Note/raw/master/Detection_Pattern_Recognition/pics/k_nearesr_neighbor.png\" style=\"zoom: 67%;\" />\n\n<p>Note:</p>\n<ul>\n<li>simple and very robust</li>\n<li>k=1 always leads to overfitting</li>\n<li>multi-class classifier</li>\n<li>search for nearest neighbor is super expensive, need special data structure like kd-tree, octree to speed up.</li>\n</ul>\n<h2 id=\"Bayes-plug-in\"><a href=\"#Bayes-plug-in\" class=\"headerlink\" title=\"Bayes plug-in\"></a>Bayes plug-in</h2><h3 id=\"Gaussian-Classifier\"><a href=\"#Gaussian-Classifier\" class=\"headerlink\" title=\"Gaussian Classifier\"></a>Gaussian Classifier</h3><img src=\"https://github.com/dbddqy/Note/raw/master/Detection_Pattern_Recognition/pics/gaussian_classifier.png\" style=\"zoom:80%;\" />\n\n<p>Note:</p>\n<ul>\n<li>becomes nearest mean if all classes have identical C and $\\mu$</li>\n<li>need lots of data, usually around 10 times the vector length d</li>\n<li>fails in <strong>non-linear-separable</strong> dataset.</li>\n</ul>\n<h3 id=\"Naive-Gaussian\"><a href=\"#Naive-Gaussian\" class=\"headerlink\" title=\"Naive Gaussian\"></a>Naive Gaussian</h3><p>just assume C is diagonal, reduce training time and amount of required data.</p>\n<h3 id=\"Gaussian-Mixture-Model-GMM\"><a href=\"#Gaussian-Mixture-Model-GMM\" class=\"headerlink\" title=\"Gaussian Mixture Model (GMM)\"></a>Gaussian Mixture Model (GMM)</h3><img src=\"https://github.com/dbddqy/Note/raw/master/Detection_Pattern_Recognition/pics/GMM_0.png\" style=\"zoom: 80%;\" />\n\n<img src=\"https://github.com/dbddqy/Note/raw/master/Detection_Pattern_Recognition/pics/GMM_1.png\" style=\"zoom:80%;\" />\n\n<p>Note:</p>\n<ul>\n<li>versatile, can approximate many real-life distributions</li>\n<li>sensitive to the choice or estimation of model orders Mj</li>\n<li>nonconvex optimization, possible convergence to local optimum, sensitive to initialization of EM</li>\n</ul>\n<h2 id=\"Discriminant-Function\"><a href=\"#Discriminant-Function\" class=\"headerlink\" title=\"Discriminant Function\"></a>Discriminant Function</h2><p>This approach can be considered as directly modeling posteriori with estimating the likelihood.</p>\n<h3 id=\"Neural-Network\"><a href=\"#Neural-Network\" class=\"headerlink\" title=\"Neural Network\"></a>Neural Network</h3><p>see course deep learning</p>\n<h3 id=\"Support-Vector-Machine\"><a href=\"#Support-Vector-Machine\" class=\"headerlink\" title=\"Support Vector Machine\"></a>Support Vector Machine</h3><p>basic ideas:</p>\n<ul>\n<li>binary classifier</li>\n<li>use a linear discriminant function</li>\n</ul>\n<p>new ideas:</p>\n<ul>\n<li>non-linear feature mapping (kernel functions)</li>\n<li>maximum margin (instead of least squares)</li>\n<li>convex optimization</li>\n</ul>\n<p><strong>Hard margin SVM:</strong></p>\n<img src=\"https://github.com/dbddqy/Note/raw/master/Detection_Pattern_Recognition/pics/hard_SVM.png\" style=\"zoom:80%;\" />\n\n<p>Note:</p>\n<ul>\n<li>dataset <strong>must be linear separable</strong></li>\n</ul>\n<p><strong>Soft margin SVM:</strong></p>\n<img src=\"https://github.com/dbddqy/Note/raw/master/Detection_Pattern_Recognition/pics/soft_SVM.png\" style=\"zoom:80%;\" />\n\n<p><img src=\"https://github.com/dbddqy/Note/raw/master/Detection_Pattern_Recognition/pics/soft_SVM_2.png\" alt=\"\"></p>\n<p>Note:</p>\n<ul>\n<li>can solve significantly larger set of problems</li>\n<li>sensitive to the choice of hyperparameters $\\gamma$ and C. They have to be optimized</li>\n</ul>\n<p><strong>Multi-class SVM:</strong> </p>\n<ol>\n<li><p>one against one</p>\n<ul>\n<li>need to train $\\tbinom{c}{2}$ SVMs for all pairs of class</li>\n<li>the class with most wins wins</li>\n</ul>\n</li>\n<li><p>one against the rest</p>\n<ul>\n<li>need to train c SVMs</li>\n<li>choose the class with the highest f(x)</li>\n</ul>\n</li>\n<li><p>hierarchical</p>\n</li>\n</ol>\n<h2 id=\"Validation\"><a href=\"#Validation\" class=\"headerlink\" title=\"Validation\"></a>Validation</h2><p>k-fold cross validation: (k − 2)/1/1 folds for training/validation/test</p>\n","categories":["theory"],"tags":["detection and pattern recognition"]},{"title":"[Theory] [Detection and Pattern Recognition] ch3","url":"https://dbddqy.github.io/2019/11/04/t-detection_pattern_recognition_ch3/","content":"<h1 id=\"Detection\"><a href=\"#Detection\" class=\"headerlink\" title=\"Detection\"></a>Detection</h1><h2 id=\"Precision-and-Recall\"><a href=\"#Precision-and-Recall\" class=\"headerlink\" title=\"Precision and Recall\"></a>Precision and Recall</h2><p>Precision = TP / (TP+FP)</p>\n<p>Recall = TP / (TP+FN)</p>\n<p>F-Score = 2/(1/P+1/R) (harmonic mean of P and R</p>\n<p><strong>ROC Curve:</strong> </p>\n<p>when moving the decision boundary to increase the detection rate, false alarm rate also increases. So the area under ROC curved to judge how good the model is. </p>\n<img src=\"https://github.com/dbddqy/Note/raw/master/Detection_Pattern_Recognition/pics/ROC.png\" style=\"zoom: 80%;\" />\n\n<h2 id=\"Practical-Challenge-Unknown-Likelihood\"><a href=\"#Practical-Challenge-Unknown-Likelihood\" class=\"headerlink\" title=\"Practical Challenge: Unknown Likelihood\"></a>Practical Challenge: Unknown Likelihood</h2><p>To have d good detection model, a precise mathematical model for the likelihood is required. In many practical cases it is difficult, so learning based approach (Pattern Recognition) becomes a good alternative.</p>\n<h2 id=\"Pattern-recognition-approaches\"><a href=\"#Pattern-recognition-approaches\" class=\"headerlink\" title=\"Pattern recognition approaches\"></a>Pattern recognition approaches</h2><p><img src=\"https://github.com/dbddqy/Note/raw/master/Detection_Pattern_Recognition/pics/pattern_recognition.png\" alt=\"\"></p>\n","categories":["theory"],"tags":["detection and pattern recognition"]},{"title":"[Theory] [Detection and Pattern Recognition] ch2","url":"https://dbddqy.github.io/2019/11/02/t-detection_pattern_recognition_ch2/","content":"<h1 id=\"Bayesian-Decision-Theory\"><a href=\"#Bayesian-Decision-Theory\" class=\"headerlink\" title=\"Bayesian Decision Theory\"></a>Bayesian Decision Theory</h1><h2 id=\"Probabilistic-Signal-Model\"><a href=\"#Probabilistic-Signal-Model\" class=\"headerlink\" title=\"Probabilistic Signal Model\"></a>Probabilistic Signal Model</h2><p>$\\omega \\in \\lbrace \\omega_1,\\omega_2,…,\\omega_c\\rbrace$ : unknown true class. </p>\n<p>$\\underline{x} \\in \\mathbb{R}^d$ : feature vector</p>\n<p>$P(\\omega=\\omega_i) = P(\\omega_i)$ : a priori probability</p>\n<p>$P(\\underline{x}|\\omega=\\omega_i) = P(\\underline{x}|\\omega_i)$ : likelihood</p>\n<p>$P(\\underline{x},\\omega=\\omega_i) = P(\\underline{x},\\omega_i)$ : joint PDF of x and w</p>\n<p>$P(\\underline{x})$ : marginal PDF of x, called evidence</p>\n<p>$P(\\omega=\\omega_i|\\underline{x}) = P(\\omega_i|\\underline{x})$ : a posteriori probability</p>\n<h2 id=\"Bayesian-Theorem\"><a href=\"#Bayesian-Theorem\" class=\"headerlink\" title=\"Bayesian Theorem:\"></a>Bayesian Theorem:</h2><p>$$<br>P(\\omega_i|\\underline{x}) = \\frac{P(\\underline{x}|\\omega_i)P(\\omega_i)}{P(\\underline{x})}<br>$$</p>\n<h2 id=\"Loss-Matrix\"><a href=\"#Loss-Matrix\" class=\"headerlink\" title=\"Loss Matrix:\"></a>Loss Matrix:</h2><p>$$<br>\\underline{\\underline{L}} = [l_{ij}]<em>{c\\times c} \\\\<br>l</em>{ij}=loss(w=i,\\hat{w}=j)<br>$$</p>\n<p>Of course diagonal elements are 0, means correct decision.</p>\n<h2 id=\"Minimum-Bayesian-Risk-MBR\"><a href=\"#Minimum-Bayesian-Risk-MBR\" class=\"headerlink\" title=\"Minimum Bayesian Risk (MBR):\"></a>Minimum Bayesian Risk (MBR):</h2><p>Bayesian Risk = Loss Matrix * Posterior<br>$$<br>\\left[ \\begin{array}{c}<br>R(\\hat{\\omega}=\\omega_1|\\underline{x}) \\<br>… \\<br>R(\\hat{\\omega}=\\omega_c|\\underline{x})<br>\\end{array}<br>\\right ] = \\underline{\\underline{L}}<br>\\left[ \\begin{array}{c}<br>P(\\hat{\\omega}=\\omega_1|\\underline{x}) \\``<br>… \\<br>P(\\hat{\\omega}=\\omega_c|\\underline{x})<br>\\end{array}<br>\\right ] \\\\<br>$$</p>\n<h2 id=\"Maximum-A-Posteriori-MAP\"><a href=\"#Maximum-A-Posteriori-MAP\" class=\"headerlink\" title=\"Maximum A Posteriori (MAP):\"></a>Maximum A Posteriori (MAP):</h2><p>Assuming 0-1 loss, MAP is special case of MBR.</p>\n<h2 id=\"Maximum-Likelihood-ML\"><a href=\"#Maximum-Likelihood-ML\" class=\"headerlink\" title=\"Maximum Likelihood (ML)\"></a>Maximum Likelihood (ML)</h2><p>With equal priors, ML is special case of MAP.</p>\n","categories":["theory"],"tags":["detection and pattern recognition"]},{"title":"[Project] Sechs (ongoing)","url":"https://dbddqy.github.io/2019/10/01/p-sechs/","content":"<h1 id=\"Sechs\"><a href=\"#Sechs\" class=\"headerlink\" title=\"Sechs\"></a>Sechs</h1><p><strong>(A collaboration with An Mo)</strong></p>\n<p><a href=\"https://github.com/moanan/sechs\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/Github-Repo-blue\" alt=\"\"></a></p>\n<p><img src=\"/pics/p-sechs/A1-A5_20200706.jpg\" alt=\"\"></p>\n<p><strong>Towards a high performance force-aware 6-axis robot!</strong></p>\n<p><img src=\"/pics/p-sechs/force_feedback.gif\" alt=\"\"></p>\n<hr>\n<h2 id=\"Design-Requirements\"><a href=\"#Design-Requirements\" class=\"headerlink\" title=\"Design Requirements\"></a>Design Requirements</h2><ul>\n<li>6 axis</li>\n<li>High performance<ul>\n<li>Joint acceleration: 2π [rad/s] (except J1: 1π [rad/s])</li>\n<li>Tool payload: 2 [kg]</li>\n</ul>\n</li>\n<li>Robot teaching by hand (force-aware)</li>\n<li>Desktop size</li>\n<li>3d printing for most parts</li>\n</ul>\n<hr>\n<h2 id=\"DH-Parameters\"><a href=\"#DH-Parameters\" class=\"headerlink\" title=\"DH Parameters\"></a>DH Parameters</h2><p>The parameters listed here are not final.</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Axis</th>\n<th align=\"left\">d</th>\n<th align=\"left\">theta</th>\n<th align=\"left\">a</th>\n<th align=\"left\">alpha</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">1</td>\n<td align=\"left\">0.112</td>\n<td align=\"left\">0.0</td>\n<td align=\"left\">0.0</td>\n<td align=\"left\">0.0</td>\n</tr>\n<tr>\n<td align=\"left\">2</td>\n<td align=\"left\">0.0</td>\n<td align=\"left\">-0.5 * pi</td>\n<td align=\"left\">0.0</td>\n<td align=\"left\">-0.5 * pi</td>\n</tr>\n<tr>\n<td align=\"left\">3</td>\n<td align=\"left\">-0.0525</td>\n<td align=\"left\">0.5 * pi</td>\n<td align=\"left\">0.2</td>\n<td align=\"left\">0.0</td>\n</tr>\n<tr>\n<td align=\"left\">4</td>\n<td align=\"left\">0.2</td>\n<td align=\"left\">0.0</td>\n<td align=\"left\">0.0</td>\n<td align=\"left\">0.5 * pi</td>\n</tr>\n<tr>\n<td align=\"left\">5</td>\n<td align=\"left\">0.085</td>\n<td align=\"left\">0.0</td>\n<td align=\"left\">0.0</td>\n<td align=\"left\">-0.5 * pi</td>\n</tr>\n<tr>\n<td align=\"left\">6</td>\n<td align=\"left\">0.08</td>\n<td align=\"left\">0.0</td>\n<td align=\"left\">0.0</td>\n<td align=\"left\">0.5 * pi</td>\n</tr>\n</tbody></table>\n<hr>\n<h2 id=\"Load-Estimation\"><a href=\"#Load-Estimation\" class=\"headerlink\" title=\"Load Estimation\"></a>Load Estimation</h2><p>The python-based <a href=\"https://github.com/moanan/sechs/tree/master/1-calculation/torqueLoadCase\" target=\"_blank\" rel=\"noopener\">Dynamic Model</a> is built for estimating required torque for each joint.</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Axis</th>\n<th align=\"left\">Required Torque[Nm]</th>\n<th align=\"left\">Motor</th>\n<th align=\"left\">Motor Torque[Nm]</th>\n<th align=\"left\">Reduction</th>\n<th align=\"left\">Output Torque[Nm]</th>\n<th align=\"left\">Safety Factor</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">1</td>\n<td align=\"left\">3.4</td>\n<td align=\"left\">Sunnysky X4110S 170kv</td>\n<td align=\"left\">~2</td>\n<td align=\"left\">4:1 (belt)</td>\n<td align=\"left\">8</td>\n<td align=\"left\">2.3</td>\n</tr>\n<tr>\n<td align=\"left\">2</td>\n<td align=\"left\">28.8</td>\n<td align=\"left\">SK3 - 4250-350kv</td>\n<td align=\"left\">1.18</td>\n<td align=\"left\">30:1</td>\n<td align=\"left\">35.4</td>\n<td align=\"left\">1.2</td>\n</tr>\n<tr>\n<td align=\"left\">3</td>\n<td align=\"left\">11.7</td>\n<td align=\"left\">SK3 - 4250-350kv</td>\n<td align=\"left\">1.18</td>\n<td align=\"left\">15:1</td>\n<td align=\"left\">17.7</td>\n<td align=\"left\">1.5</td>\n</tr>\n<tr>\n<td align=\"left\">4</td>\n<td align=\"left\">2.6</td>\n<td align=\"left\">T-Motor AS 2820 880kv</td>\n<td align=\"left\">0.3</td>\n<td align=\"left\">19.2:1</td>\n<td align=\"left\">5.76</td>\n<td align=\"left\">2.2</td>\n</tr>\n<tr>\n<td align=\"left\">5</td>\n<td align=\"left\">3.9</td>\n<td align=\"left\">T-Motor AS 2820 880kv</td>\n<td align=\"left\">0.3</td>\n<td align=\"left\">19.2:1</td>\n<td align=\"left\">5.76</td>\n<td align=\"left\">1.5</td>\n</tr>\n<tr>\n<td align=\"left\">6</td>\n<td align=\"left\">N/A</td>\n<td align=\"left\">T-Motor AS 2820 880kv</td>\n<td align=\"left\">0.3</td>\n<td align=\"left\">19.2:1</td>\n<td align=\"left\">5.76</td>\n<td align=\"left\">N/A</td>\n</tr>\n</tbody></table>\n<hr>\n<h2 id=\"Mechanical-Design-20200706\"><a href=\"#Mechanical-Design-20200706\" class=\"headerlink\" title=\"Mechanical Design 20200706\"></a>Mechanical Design 20200706</h2><p><img src=\"/pics/p-sechs/mechanical_design_20200706.png\" alt=\"\"></p>\n<hr>\n<h2 id=\"Testing-20200812\"><a href=\"#Testing-20200812\" class=\"headerlink\" title=\"Testing 20200812\"></a>Testing 20200812</h2><div align=\"left\">\n    <iframe src=\"https://player.vimeo.com/video/447085333\" width=\"640\" height=\"320\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen></iframe>\n    <p><a href=\"https://vimeo.com/447085333\" target=\"_blank\" rel=\"noopener\">Testing Force FeedBack</a></p>\n</div>\n\n<h2 id=\"Testing-20200716\"><a href=\"#Testing-20200716\" class=\"headerlink\" title=\"Testing 20200716\"></a>Testing 20200716</h2><div align=\"left\">\n    <iframe src=\"https://player.vimeo.com/video/443796757\" width=\"320\" height=\"640\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen></iframe>\n    <p><a href=\"https://vimeo.com/443796757\" target=\"_blank\" rel=\"noopener\">Testing Jogging</a></p>\n</div>\n\n<div align=\"left\">\n    <iframe src=\"https://player.vimeo.com/video/443797208\" width=\"320\" height=\"640\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen></iframe>\n    <p><a href=\"https://vimeo.com/443797208\" target=\"_blank\" rel=\"noopener\">Testing Teaching</a></p>\n</div>\n\n<div align=\"left\">\n    <iframe src=\"https://player.vimeo.com/video/443797861\" width=\"320\" height=\"640\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen></iframe>\n    <p><a href=\"https://vimeo.com/443797861\" target=\"_blank\" rel=\"noopener\">Testing Offline Planning</a></p>\n</div>\n","categories":["project"],"tags":["robotics","ongoing"]},{"title":"[Theory] [Intro to Robotics, Mechanics and Control] ch5","url":"https://dbddqy.github.io/2019/09/23/t-intro_to_robotics_ch5/","content":"<h1 id=\"Jacobians-velocities-and-static-forces\"><a href=\"#Jacobians-velocities-and-static-forces\" class=\"headerlink\" title=\"Jacobians: velocities and static forces\"></a>Jacobians: velocities and static forces</h1><h3 id=\"Relative-Velocity\"><a href=\"#Relative-Velocity\" class=\"headerlink\" title=\"Relative Velocity\"></a>Relative Velocity</h3><p>${}^A ({}^B V_{Q})$ is the velocity of Q relative to {B}, described in {A}</p>\n<p>${}^A ({}^A V_{Q})$ can be written as ${}^A V_{Q}$</p>\n<p>${}^U V_{CORG}$ can be written as $v_c$ (U is the world frame)</p>\n<h3 id=\"Observation-of-motion-in-B-from-A\"><a href=\"#Observation-of-motion-in-B-from-A\" class=\"headerlink\" title=\"Observation of motion in {B} from {A}\"></a>Observation of motion in {B} from {A}</h3><p>$$<br>{}^AV_Q={}^AV_{BORG}+{}^A_B R {}^B V_Q + {}^A \\Omega_B \\times {}^A_B R {}^B Q \\tag{5-13}<br>$$</p>\n<p>three parts:</p>\n<ul>\n<li>velocity of {B} origin relative to {A}</li>\n<li>velocity of Q relative to {B}, described in {A}</li>\n<li>effect caused by rotation of {B}, relative to {A}</li>\n</ul>\n<h3 id=\"Velocity-“propagation”\"><a href=\"#Velocity-“propagation”\" class=\"headerlink\" title=\"Velocity “propagation”\"></a>Velocity “propagation”</h3><p>a special case of (5-13)</p>\n<p>$$<br>{}^{i+1}\\omega_{i+1} = {}^{i+1}_ iR {}^i\\omega_i + \\dot{\\theta} _{i+1} {}^{i+1}\\hat{Z} _{i+1} \\tag{5-45}<br>$$</p>\n<p>$$<br>{}^{i+1}v_{i+1} = {}^{i+1}_ iR({}^iv_i+{}^i\\omega_i \\times {}^iP_{i+1}) \\tag{5-46}<br>$$</p>\n<h3 id=\"Jacobian-6-N\"><a href=\"#Jacobian-6-N\" class=\"headerlink\" title=\"Jacobian (6*N)\"></a>Jacobian (6*N)</h3><p>$$<br>\\begin{bmatrix} v \\\\ \\omega \\end{bmatrix} = J(\\theta)\\dot{\\theta} \\tag{5-64}<br>$$</p>\n<p>$$<br>J(\\theta) = \\begin{bmatrix}<br>\\hat{Z}_0 \\times (O_N - O_0) &amp; \\hat{Z}_1 \\times (O_N - O_1) &amp; \\cdots \\\\<br>\\hat{Z}_0 &amp; \\hat{Z}_1 &amp; \\cdots<br>\\end{bmatrix}<br>$$</p>\n<p>transform 6*6 jacobian from {B} to {A}:<br>$$<br>{}^A  J(\\theta) = \\begin{bmatrix} {}^A_B  R &amp; 0 \\\\ 0 &amp; {}^A_B  R \\end{bmatrix} {}^B  J(\\theta) \\tag{5-71}<br>$$</p>\n<h3 id=\"Singularity\"><a href=\"#Singularity\" class=\"headerlink\" title=\"Singularity\"></a>Singularity</h3><p>$$<br>det(J(\\theta)) = 0<br>$$</p>\n<ul>\n<li>Workspace-boundary singularities: when it is fully stretched out or folded back</li>\n<li>Workspace-interior singularities: when multiple joints line up</li>\n</ul>\n<h3 id=\"Static-force\"><a href=\"#Static-force\" class=\"headerlink\" title=\"Static force\"></a>Static force</h3><p>According to the principle of virtual work:<br>$$<br>\\tau = J^T \\mathcal{F}<br>$$<br>$\\mathcal{F}$ consists of force and moment at the end, which can be mapped to joint torque by jacobian transpose</p>\n","categories":["theory"],"tags":["Intro to Robotics, Mechanics and Control","robotics"]},{"title":"[Theory] [Intro to Robotics, Mechanics and Control] ch4","url":"https://dbddqy.github.io/2019/09/12/t-intro_to_robotics_ch4/","content":"<h1 id=\"Inverse-manipulator-kinematics\"><a href=\"#Inverse-manipulator-kinematics\" class=\"headerlink\" title=\"Inverse manipulator kinematics\"></a>Inverse manipulator kinematics</h1><p>Dextrous workspace: </p>\n<blockquote>\n<p>the volume of space that the robot end-effector can reach with all orientations</p>\n</blockquote>\n<p>Reachable workspace:</p>\n<blockquote>\n<p>the volume of space that the robot can reach in at least one orientation.</p>\n</blockquote>\n<p>Multiple solutions:</p>\n<ul>\n<li>choose the closest one</li>\n<li>or consider collision when there is obstacle</li>\n</ul>\n<h3 id=\"Analytical-Solution-Closed-Form\"><a href=\"#Analytical-Solution-Closed-Form\" class=\"headerlink\" title=\"Analytical Solution (Closed-Form)\"></a>Analytical Solution (Closed-Form)</h3><p>Possible only when many alphas are 0 or 90 degrees.</p>\n<p>Wrist-partition robot always has closed-form solution.</p>\n<p>Has to be solved individually by each specific type of robot.</p>\n<h3 id=\"Numerical-Solution\"><a href=\"#Numerical-Solution\" class=\"headerlink\" title=\"Numerical Solution\"></a>Numerical Solution</h3><ul>\n<li>Jacobian Pseudo Inverse: need to calculated inverse, sensitive to singularity, but convergent very fast</li>\n<li>Jacobian Transpose: robust around singularity, but need way more iterations to convergent</li>\n</ul>\n","categories":["theory"],"tags":["Intro to Robotics, Mechanics and Control","robotics"]},{"title":"[Theory] [Intro to Robotics, Mechanics and Control] ch3","url":"https://dbddqy.github.io/2019/08/26/t-intro_to_robotics_ch3/","content":"<h1 id=\"Manipulator-Kinematics\"><a href=\"#Manipulator-Kinematics\" class=\"headerlink\" title=\"Manipulator Kinematics\"></a>Manipulator Kinematics</h1><h3 id=\"DH-Parameters-modified\"><a href=\"#DH-Parameters-modified\" class=\"headerlink\" title=\"DH Parameters (modified)\"></a>DH Parameters (modified)</h3><p>Procedures to set up frames all each joint:</p>\n<ol>\n<li>Identify the joint axes</li>\n<li>Identify the common perpendicular between them, choose its intersection with i-th axis as origin of {i}. (if parallel, any point can be chosen )</li>\n<li>Assign z along i-th axis</li>\n<li>Assign x along common perpendicular</li>\n<li>Assign y to complete a right-hand coordinate system</li>\n<li>Assign {0} to match {1} when the first joint variable is zero. For {N}, choose an origin location and x direction freely, but generally so as to cause as many linkage parameters as possible to become zero.</li>\n</ol>\n<p>Parameters:</p>\n<ul>\n<li>a: translate along x, align z</li>\n<li>alpha: rotate around x, align z</li>\n<li>d: translate along z, align origin</li>\n<li>theta: rotate around z, align x</li>\n</ul>\n<p>Transformation matrix formula see book (3-6)</p>\n<p><strong>Get forward kinematics formula by multiplying all transformation matrix together.</strong></p>\n","categories":["theory"],"tags":["Intro to Robotics, Mechanics and Control","robotics"]},{"title":"[Theory] [Advanced Math ISS] ch4","url":"https://dbddqy.github.io/2019/08/20/t-math_iss_ch4/","content":"<h1 id=\"Random-Variable\"><a href=\"#Random-Variable\" class=\"headerlink\" title=\"Random Variable\"></a>Random Variable</h1><h2 id=\"Definition\"><a href=\"#Definition\" class=\"headerlink\" title=\"Definition\"></a>Definition</h2><p>RV is a function $X=X(\\xi)$ that maps any outcome $\\xi$ to a real number.</p>\n<h2 id=\"CDF-Cumulative-Distribution-Function\"><a href=\"#CDF-Cumulative-Distribution-Function\" class=\"headerlink\" title=\"CDF (Cumulative Distribution Function)\"></a>CDF (Cumulative Distribution Function)</h2><p>$$<br>F_X(x) = P(X\\leq x)=P(\\xi|X(\\xi)\\leq x)<br>$$</p>\n<p>It is continuous from the right.</p>\n<h2 id=\"PDF-Probability-Density-Function\"><a href=\"#PDF-Probability-Density-Function\" class=\"headerlink\" title=\"PDF (Probability Density Function)\"></a>PDF (Probability Density Function)</h2><p>$$<br>f_X(x) = \\frac{d}{dx}F_X(x)<br>\\\\ F_X(x) = \\int_{-\\infty}^{x}f_X(u)du<br>$$</p>\n<p>For discrete RV, PDF consists of Dirac functions:<br>$$<br>\\delta(x) = \\begin{cases}<br>     0， &amp;x\\ne0<br>\\\\ \\infty， &amp;x=0<br>\\end{cases}<br>$$<br>Properties of Dirac function:<br>$$<br>\\begin{align} \\int_{-\\infty}^{\\infty}\\delta(x)dx &amp;= 1<br>\\\\ \\int_{-\\infty}^{\\infty}f(x)\\delta(x-x_0)dx &amp;= f(x_0)<br>\\end{align}<br>$$</p>\n<h2 id=\"Gaussian-Distribution\"><a href=\"#Gaussian-Distribution\" class=\"headerlink\" title=\"Gaussian Distribution\"></a>Gaussian Distribution</h2><p>$$<br>X \\thicksim N(\\mu, \\sigma^2)<br>\\\\ f_X(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}exp\\lbrace{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\rbrace<br>$$</p>\n<h2 id=\"Bivariate-CDF-and-PDF\"><a href=\"#Bivariate-CDF-and-PDF\" class=\"headerlink\" title=\"Bivariate CDF and PDF\"></a>Bivariate CDF and PDF</h2><p>$$<br>F_{XY}(x,y)=P(X\\leq x,Y\\leq y)<br>\\\\  f_XY(x,y)= \\frac{\\partial^2F_{XY}(x,y)}{\\partial x \\partial y}<br>$$</p>\n<p>“$F_{XY}(x,y)$ starts with 0, increases two-dimensionally and ends up with 1.”</p>\n<p>“$f_{XY}(x,y)$ is a nonnegative function with volume 1.”</p>\n<h2 id=\"Marginal-CDF-and-PDF\"><a href=\"#Marginal-CDF-and-PDF\" class=\"headerlink\" title=\"Marginal CDF and PDF\"></a>Marginal CDF and PDF</h2><p>CDF: set unneeded variable to $\\infty$ :<br>$$<br>F_X(x) = F_{XY}(x, \\infty)<br>\\\\  F_Y(y) = F_{XY}(\\infty, y)<br>$$<br>PDF: integrate over unneeded variable:<br>$$<br>f_X(x) = \\int_{-\\infty}^{\\infty}F_{XY}(x,y)dy<br>\\\\  f_Y(y) = \\int_{-\\infty}^{\\infty}F_{XY}(x,y)dx<br>$$</p>\n<h2 id=\"Multivariate-Gaussian\"><a href=\"#Multivariate-Gaussian\" class=\"headerlink\" title=\"Multivariate Gaussian\"></a>Multivariate Gaussian</h2><p>$$<br>\\boldsymbol{X} \\thicksim N(\\boldsymbol{\\mu}, \\boldsymbol{C})<br>\\\\  f_{\\boldsymbol{X}}(\\boldsymbol{x}) = \\frac{1}{(2\\pi)^{N/2}det(C)^{1/2}}exp\\lbrace{-\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu})^T\\boldsymbol{C}^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu})}\\rbrace<br>$$</p>\n<p>if <strong><em>C</em></strong> = <strong><em>I</em></strong>:<br>$$<br>f_{\\boldsymbol{X}}(\\boldsymbol{x}) = \\frac{1}{(2\\pi)^{N/2}}exp\\lbrace{-\\frac{1}{2}|\\boldsymbol{x}|^2}\\rbrace<br>$$</p>\n<h2 id=\"Conditional-CDF-and-PDF-Given-Event\"><a href=\"#Conditional-CDF-and-PDF-Given-Event\" class=\"headerlink\" title=\"Conditional CDF and PDF Given Event\"></a>Conditional CDF and PDF Given Event</h2><p>CDF of X given an event B:<br>$$<br>F_X(x|B) = P(X \\leq x|B)=\\frac{P(X\\leq x \\cap B)}{P(B)}<br>$$<br>PDF:<br>$$<br>f_X(x|B) = \\frac{d}{dx}F_X(x|B)<br>$$<br>They share same properties as for the conditional probability.</p>\n<p>Law of total CDF/PDF: if<br>$$<br>B_i \\cap B_j \\neq \\empty, \\forall i \\neq j<br>\\\\  \\cup_{i}^{}{B_i} = \\Omega<br>$$<br>then<br>$$<br>F_X(x) = \\sum_i F_X(x|B_i)P(B_i)<br>\\\\  f_X(x) = \\sum_i f_X(x|B_i)P(B_i)<br>$$<br>Bayes rule:<br>$$<br>F_X(x|B) = P(B|X \\leq x) \\frac{F_X(x)}{P(B)}<br>\\\\  f_X(x|B) = P(B|X = x) \\frac{f_X(x)}{P(B)}<br>$$</p>\n<h2 id=\"Conditional-CDF-and-PDF-Given-PDF-CDF\"><a href=\"#Conditional-CDF-and-PDF-Given-PDF-CDF\" class=\"headerlink\" title=\"Conditional CDF and PDF Given PDF/CDF\"></a>Conditional CDF and PDF Given PDF/CDF</h2><p>X and Y are two (continuous) random <strong>vector</strong>. Consider condition B is event {Y|Y&lt;y}, then conditional CDF of X given Y:<br>$$<br>F_\\boldsymbol{X}(\\boldsymbol{x}|\\boldsymbol{Y}&lt;\\boldsymbol{y})=P(\\boldsymbol{X}&lt;\\boldsymbol{x}|\\boldsymbol{Y}&lt;\\boldsymbol{y})=\\frac{F_{\\boldsymbol{X},\\boldsymbol{Y}}(\\boldsymbol{x},\\boldsymbol{y})}{F_\\boldsymbol{Y}(\\boldsymbol{y})}<br>$$<br>PDF can be given with B = {y &lt; Y &lt; y + dy}, followed by dy-&gt;0:<br>$$<br>f_\\boldsymbol{X}(\\boldsymbol{x}|\\boldsymbol{Y}=\\boldsymbol{y})=\\frac{f_{\\boldsymbol{X},\\boldsymbol{Y}}(\\boldsymbol{x},\\boldsymbol{y})}{f_\\boldsymbol{Y}(\\boldsymbol{y})}<br>$$<br>And Bayes rule:<br>$$<br>f_\\boldsymbol{X}(\\boldsymbol{x}|\\boldsymbol{Y}=\\boldsymbol{y})=f_\\boldsymbol{Y}(\\boldsymbol{y}|\\boldsymbol{X}=\\boldsymbol{x})\\frac{f_{\\boldsymbol{X},\\boldsymbol{Y}}(\\boldsymbol{x},\\boldsymbol{y})}{f_\\boldsymbol{Y}(\\boldsymbol{y})}<br>$$</p>\n<h2 id=\"Independent-Random-Vector\"><a href=\"#Independent-Random-Vector\" class=\"headerlink\" title=\"Independent Random Vector\"></a>Independent Random Vector</h2><p>X and Y are independent:<br>$$<br>F_{\\boldsymbol{X},\\boldsymbol{Y}}(\\boldsymbol{x},\\boldsymbol{y})=F_\\boldsymbol{X}(\\boldsymbol{x})F_\\boldsymbol{Y}(\\boldsymbol{y})<br>\\\\  f_{\\boldsymbol{X},\\boldsymbol{Y}}(\\boldsymbol{x},\\boldsymbol{y})=f_\\boldsymbol{X}(\\boldsymbol{x})f_\\boldsymbol{Y}(\\boldsymbol{y})<br>$$<br>Properties:</p>\n<p>P1: Conditional PDF is the same as unconditional ones.</p>\n<p>P2: h(x) and g(y) also independent.<br>$$<br>f_\\boldsymbol{X}(\\boldsymbol{x}|\\boldsymbol{y})=f_\\boldsymbol{X}(\\boldsymbol{x})<br>\\\\  f_{\\boldsymbol{X},\\boldsymbol{Y}}(h(\\boldsymbol{x}),g(\\boldsymbol{y}))=f_\\boldsymbol{X}(h(\\boldsymbol{x}))f_\\boldsymbol{Y}(g(\\boldsymbol{y}))<br>$$</p>\n<h2 id=\"Expectation\"><a href=\"#Expectation\" class=\"headerlink\" title=\"Expectation\"></a>Expectation</h2><p>$$<br>E(X)= \\int xF_X(x)dx<br>$$</p>\n<h2 id=\"Moments\"><a href=\"#Moments\" class=\"headerlink\" title=\"Moments\"></a>Moments</h2><p>$\\boldsymbol{\\mu}=E(\\boldsymbol{X})$ : mean vector of <strong><em>X</em></strong></p>\n<p>$r_{ij}=E(X_iX_j)$ : correlation between $X_i$ and $X_j$</p>\n<p>$r_{ii}=E(X_i^2)$ : 2. moment of $X_i$</p>\n<p>$\\boldsymbol{R}=E(\\boldsymbol{X}\\boldsymbol{X}^T)$ : correlation matrix of <strong><em>X</em></strong></p>\n<p>$c_{ij}=E((X_i-\\mu_i)(X_j-\\mu_j))$ : covariance between $X_i$ and $X_j$</p>\n<p>$c_{ii}=E((X_i-\\mu_i)^2)=\\sigma_i^2$ : variance of $X_i$ </p>\n<p>$\\boldsymbol{C}=E((\\boldsymbol{X-\\mu})(\\boldsymbol{X-\\mu})^T)$ : covariance matrix of <strong><em>X</em></strong></p>\n","categories":["theory"],"tags":["math"]},{"title":"[Theory] [Advanced Math ISS] ch3","url":"https://dbddqy.github.io/2019/08/19/t-math_iss_ch3/","content":"<h1 id=\"Probability\"><a href=\"#Probability\" class=\"headerlink\" title=\"Probability\"></a>Probability</h1><h2 id=\"Notations-and-Definitions\"><a href=\"#Notations-and-Definitions\" class=\"headerlink\" title=\"Notations and Definitions\"></a>Notations and Definitions</h2><p>Consider a random experiment, <strong>outcome</strong> $\\xi$ describes one particular result. <strong>Event</strong> <em>A</em> is a set of outcome. <strong>Sample space</strong>  $\\Omega$ is set of all possible outcomes.<br>$$<br>\\xi \\in \\Omega<br>\\\\ A \\subset \\Omega<br>$$</p>\n<h2 id=\"Conditional-Probability\"><a href=\"#Conditional-Probability\" class=\"headerlink\" title=\"Conditional Probability\"></a>Conditional Probability</h2><p>Conditional probability reduces the sample space $\\Omega$ to a subset of it, denoted as <em>B</em> here:<br>$$<br>P(A|\\Omega) = \\frac{P(A\\cap \\Omega)}{P(\\Omega)} = P(A)<br>\\\\ P(A|B) = \\frac{P(A\\cap B)}{P(B)}<br>$$</p>\n<h2 id=\"Bayes-Rule\"><a href=\"#Bayes-Rule\" class=\"headerlink\" title=\"Bayes Rule\"></a>Bayes Rule</h2><p>$$<br>P(A|B) = \\frac{P(B|A)P(A)}{P(B)}<br>$$</p>\n<h2 id=\"Law-of-Total-Probability\"><a href=\"#Law-of-Total-Probability\" class=\"headerlink\" title=\"Law of Total Probability\"></a>Law of Total Probability</h2><p>Given:<br>$$<br>B_i \\cap B_j \\neq \\empty, \\forall i \\neq j<br>\\\\  A \\subset \\cup_{i}^{}{B_i}<br>$$ {/end}<br>then:<br>$$<br>P(A) = \\sum_{i}P(A|B_i)P(B_i)<br>$$</p>\n<h2 id=\"Chain-Rule-of-Probability\"><a href=\"#Chain-Rule-of-Probability\" class=\"headerlink\" title=\"Chain Rule of Probability\"></a>Chain Rule of Probability</h2><p>$$<br>P(A_1\\cap A_2\\cap \\cdots \\cap A_N) = P(A_1|A_2\\cap \\cdots \\cap A_N)\\cdots P(A_{N-1}|A_N)P(A_N)<br>$$</p>\n<h2 id=\"Independent\"><a href=\"#Independent\" class=\"headerlink\" title=\"Independent\"></a>Independent</h2><p>A and B are independent, then:<br>$$<br>P(A|B)=P(A)<br>\\\\ P(A,B)=P(A)P(B)<br>$$<br>So if A<sub>1</sub>,  A<sub>2</sub>, …,  A<sub>N</sub> are independent, then:<br>$$<br>P(A_1\\cap A_2\\cap \\cdots \\cap A_N) = P(A_1)\\cdots P(A_{N-1})P(A_N)<br>$$</p>\n","categories":["theory"],"tags":["math"]},{"title":"[Theory] [Intro to Robotics, Mechanics and Control] ch2","url":"https://dbddqy.github.io/2019/08/18/t-intro_to_robotics_ch2/","content":"<h1 id=\"Spatial-Descriptions-and-Transformations\"><a href=\"#Spatial-Descriptions-and-Transformations\" class=\"headerlink\" title=\"Spatial Descriptions and Transformations\"></a>Spatial Descriptions and Transformations</h1><h3 id=\"Rotation-Matrix\"><a href=\"#Rotation-Matrix\" class=\"headerlink\" title=\"Rotation Matrix\"></a>Rotation Matrix</h3><p>$$<br>{}^A_BR = [ \\begin{array}{c} {}^A\\hat{X}_B &amp; ^A\\hat{Y}_B  &amp; {}^A\\hat{Z}_B \\end{array}] \\\\<br>{}^B_AR = {}^A_BR^{-1} = {}^A_BR^T<br>$$</p>\n<p>$^A\\hat{X}_B$ is the unit vector along frame B’s x direction, described in frame A</p>\n<h3 id=\"Transformation-Matrix\"><a href=\"#Transformation-Matrix\" class=\"headerlink\" title=\"Transformation Matrix\"></a>Transformation Matrix</h3><p>$$<br>\\begin{align} {}^A_BT &amp;= \\begin{bmatrix} ^A_BR &amp; {}^AP_{BORG} \\\\ 0^T &amp; 1 \\end{bmatrix} \\\\<br>{}^A_CT &amp;= {}^A_BT {}^B_CT \\\\<br>^AP &amp;= {}^A_BT ^BP \\\\<br>^AP &amp;= {}^A_BR ^BP + {}^AP_{BORG}<br>\\end{align}<br>$$</p>\n<h3 id=\"Euler-Angle\"><a href=\"#Euler-Angle\" class=\"headerlink\" title=\"Euler Angle\"></a>Euler Angle</h3><ul>\n<li>around axes of fixed reference frame</li>\n<li>around axes of rotated reference frame (itself)</li>\n<li>x-y-z fixed angles equivalent z-y-x euler angle</li>\n</ul>\n<p>formula see book (2-64)</p>\n<h3 id=\"Angle-Axis\"><a href=\"#Angle-Axis\" class=\"headerlink\" title=\"Angle-Axis\"></a>Angle-Axis</h3><p>Rodrigues’ rotation formula, see book (2-80)</p>\n<h3 id=\"Quaternion\"><a href=\"#Quaternion\" class=\"headerlink\" title=\"Quaternion\"></a>Quaternion</h3><p>$$<br>\\begin{align}<br>q_1 = k_xsin(\\theta/2) \\\\<br>q_2 = k_ysin(\\theta/2) \\\\<br>q_3 = k_zsin(\\theta/2) \\\\<br>q_4 = cos(\\theta/2) \\\\<br>\\end{align}<br>$$</p>\n<h3 id=\"Free-Vector\"><a href=\"#Free-Vector\" class=\"headerlink\" title=\"Free Vector\"></a>Free Vector</h3><p>During transformation, apply only the rotation, no translation</p>\n","categories":["theory"],"tags":["Intro to Robotics, Mechanics and Control","robotics"]},{"title":"[Project] Move-X (ongoing)","url":"https://dbddqy.github.io/2019/07/29/p-move_x/","content":"<h1 id=\"Move-X\"><a href=\"#Move-X\" class=\"headerlink\" title=\"Move-X\"></a>Move-X</h1><p><a href=\"https://github.com/dbddqy/mobile_robot\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/Github-Repo-blue\" alt=\"\"></a></p>\n<p><img src=\"https://github.com/dbddqy/mobile_robot/raw/master/pics/move_x.jpg\" alt=\"\"></p>\n<p><strong>By building a omni-directional robot platform, this project is mainly for me to learn mechanics and control, as well as robot state estimation and sensor fusion for autonomous vehicles.</strong></p>\n<h2 id=\"Low-Level\"><a href=\"#Low-Level\" class=\"headerlink\" title=\"Low Level:\"></a>Low Level:</h2><p>Mecanum wheels locomotion strategy. Using DJI Robomaster hardware.</p>\n<ul>\n<li>mechanical design</li>\n<li>mecanum wheel kinematics</li>\n<li>motor control: closed-loop velocity control</li>\n<li>publish wheel odometry and IMU data through serial communication</li>\n</ul>\n<h2 id=\"High-Level\"><a href=\"#High-Level\" class=\"headerlink\" title=\"High Level:\"></a>High Level:</h2><p>Laser-based SLAM and Navigation.</p>\n<ul>\n<li>implement slam-gmapping</li>\n<li>implement navigation related ros packages</li>\n<li>develop own solution for slam and sensor fusion<br>(combining the theoretical study of <em>Probabilistic Robotics</em> and <em>State Estimation for Robotics</em>)</li>\n</ul>\n<h2 id=\"Testing-Chassis-20200709\"><a href=\"#Testing-Chassis-20200709\" class=\"headerlink\" title=\"Testing Chassis 20200709\"></a>Testing Chassis 20200709</h2><p><img src=\"https://github.com/dbddqy/mobile_robot/raw/master/pics/test_chassis.gif\" alt=\"\"></p>\n<h2 id=\"Testing-odometry-20200710\"><a href=\"#Testing-odometry-20200710\" class=\"headerlink\" title=\"Testing odometry 20200710\"></a>Testing odometry 20200710</h2><p><img src=\"https://github.com/dbddqy/mobile_robot/raw/master/pics/test_odom.gif\" alt=\"\"></p>\n<h2 id=\"Testing-slam-gmapping-20200722\"><a href=\"#Testing-slam-gmapping-20200722\" class=\"headerlink\" title=\"Testing slam-gmapping 20200722\"></a>Testing slam-gmapping 20200722</h2><p><img src=\"https://github.com/dbddqy/mobile_robot/raw/master/pics/test_gmapping.gif\" alt=\"\"></p>\n","categories":["project"],"tags":["robotics","ongoing"]},{"title":"[Project] Robotic Censorship","url":"https://dbddqy.github.io/2019/07/21/p-robotic_censorship/","content":"<h1 id=\"Robotic-Censorship\"><a href=\"#Robotic-Censorship\" class=\"headerlink\" title=\"Robotic Censorship\"></a>Robotic Censorship</h1><p><strong>(A collaboration with Zhetao Dong, Yanan Guo and Hooman Salyani)</strong></p>\n<p><a href=\"https://github.com/dbddqy/end_effector\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/Github-Repo-blue\" alt=\"\"></a></p>\n<p><img src=\"https://github.com/dbddqy/end_effector/blob/master/pics/process.gif?raw=true\" alt=\"\"></p>\n<p>This is a final project of 2019 CDDF seminar (computational design and digital fabrication) at University of Stuttgart.</p>\n<blockquote>\n<p>This project use use robot and object recognition algorithm to apply a block of paint for censorship.</p>\n</blockquote>\n<p>The robot has a stereo camera module equipped on its end-effector for object detection and depth inference, as well as two stepper motors and gear system for applying the paint.</p>\n<p>The robot goes to some pre-defined scanning locations and takes the scan. If any human face is found, it goes above that face and paint on it, otherwise it goes to next scan location. If there is little paint left in the syringe, it performs the auto-reload.</p>\n<p><strong>Hardware:</strong></p>\n<ul>\n<li>Raspberry Pi 3B+</li>\n<li>Stereo camera module</li>\n<li>Stepper Motors</li>\n<li>Gear and rack</li>\n<li>Syringe</li>\n</ul>\n<p><strong>Software:</strong></p>\n<ul>\n<li>OpenCV (python wrapper)</li>\n<li>Tensorflow (not used in the end)</li>\n</ul>\n<p><strong>Process:</strong></p>\n<p><img src=\"https://github.com/dbddqy/end_effector/blob/master/pics/process.png?raw=true\" alt=\"\"></p>\n<p><strong>End_effector:</strong></p>\n<p><img src=\"https://github.com/dbddqy/end_effector/blob/master/pics/end_effector.png?raw=true\" alt=\"\"></p>\n<p><strong>Communication with the KUKA robot:</strong></p>\n<p><img src=\"https://github.com/dbddqy/end_effector/blob/master/pics/communication.png?raw=true\" alt=\"\"></p>\n<p><strong>Full video:</strong></p>\n<div align=\"left\">\n    <iframe src=\"https://player.vimeo.com/video/443476046\" width=\"640\" height=\"360\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen></iframe>\n    <p><a href=\"https://vimeo.com/443476046\" target=\"_blank\" rel=\"noopener\">Robotic Sensorship 01</a></p>\n</div>\n\n<div align=\"left\">\n    <iframe src=\"https://player.vimeo.com/video/443476348\" width=\"640\" height=\"360\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen></iframe>\n    <p><a href=\"https://vimeo.com/443476348\" target=\"_blank\" rel=\"noopener\">Robotic Sensorship 02</a></p>\n</div>\n","categories":["project"],"tags":["ITECH master"]},{"title":"[Exercise] Deep Neural Network in Pytorch","url":"https://dbddqy.github.io/2019/06/30/e-dnn_pytorch/","content":"<h1 id=\"Deep-Neural-Network-in-Pytorch\"><a href=\"#Deep-Neural-Network-in-Pytorch\" class=\"headerlink\" title=\"Deep Neural Network in Pytorch\"></a>Deep Neural Network in Pytorch</h1><p><a href=\"https://github.com/dbddqy/Note/tree/master/Deep_Learning/exercise_dnn_pytorch\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/Github-Source%20Code-blue\" alt=\"\"></a></p>\n<p>A network with 3 hidden layers, each contains 5 neurons.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Net</span><span class=\"params\">(nn.Module)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        super().__init__()</span><br><span class=\"line\">        self.fc1 = nn.Linear(<span class=\"number\">2</span>, <span class=\"number\">5</span>)</span><br><span class=\"line\">        self.fc2 = nn.Linear(<span class=\"number\">5</span>, <span class=\"number\">5</span>)</span><br><span class=\"line\">        self.fc3 = nn.Linear(<span class=\"number\">5</span>, <span class=\"number\">5</span>)</span><br><span class=\"line\">        self.fc4 = nn.Linear(<span class=\"number\">5</span>, <span class=\"number\">5</span>)</span><br><span class=\"line\">        self.fc_out = nn.Linear(<span class=\"number\">5</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        x = F.relu(self.fc1(x))</span><br><span class=\"line\">        x = F.relu(self.fc2(x))</span><br><span class=\"line\">        x = F.relu(self.fc3(x))</span><br><span class=\"line\">        x = F.relu(self.fc4(x))</span><br><span class=\"line\">        x = self.fc_out(x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> F.softmax(x, dim=<span class=\"number\">-1</span>)</span><br></pre></td></tr></table></figure>\n\n<p>Result:</p>\n<p><img src=\"https://github.com/dbddqy/Note/raw/master/Deep_Learning/exercise_dnn_pytorch/result.png\" alt=\"\"></p>\n<p>Result of using too many neurons (overfitting):</p>\n<p><img src=\"https://github.com/dbddqy/Note/raw/master/Deep_Learning/exercise_dnn_pytorch/overfitting.png\" alt=\"\"></p>\n","categories":["exercise"],"tags":["deep learning"]},{"title":"[Exercise] Deep Neural Network from scratch - Classification","url":"https://dbddqy.github.io/2019/06/28/e-dnn_classification/","content":"<h1 id=\"Deep-Neural-Network-from-scratch-Classification\"><a href=\"#Deep-Neural-Network-from-scratch-Classification\" class=\"headerlink\" title=\"Deep Neural Network from scratch - Classification\"></a>Deep Neural Network from scratch - Classification</h1><p><a href=\"https://github.com/dbddqy/Note/tree/master/Deep_Learning/exercise_dnn_classification\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/Github-Source%20Code-blue\" alt=\"\"></a></p>\n<p>The neural network class. Add layer by given weight matrix, bias vector, as well as activation function. Model function calculates the forward chain, while gradient function </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">NeuralNetwork</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">add_dense_layer</span><span class=\"params\">(self, n, activation=relu, d_activation=d_relu)</span>:</span></span><br><span class=\"line\">        self.Ws.append(np.random.normal(<span class=\"number\">0.</span>, <span class=\"number\">2.</span>/self.dims[<span class=\"number\">-1</span>], [n, self.dims[<span class=\"number\">-1</span>]]))</span><br><span class=\"line\">        self.Bs.append(np.zeros([n, <span class=\"number\">1</span>]))</span><br><span class=\"line\">        self.activations.append(activation)</span><br><span class=\"line\">        self.d_activations.append(d_activation)</span><br><span class=\"line\">        self.dims.append(n)</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">model</span><span class=\"params\">(self, x0)</span>:</span></span><br><span class=\"line\">        x_temp = x0</span><br><span class=\"line\">        x = [x_temp]</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(self.Ws)):</span><br><span class=\"line\">            x_temp = self.activations[i](self.Ws[i].dot(x_temp) + self.Bs[i])</span><br><span class=\"line\">            x.append(x_temp)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">gradient</span><span class=\"params\">(self, xs, ys, d_loss_f)</span>:</span></span><br><span class=\"line\">        grad_w, grad_b = [], []</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(self.Ws)):</span><br><span class=\"line\">            grad_w.append(np.zeros(self.Ws[i].shape))</span><br><span class=\"line\">            grad_b.append(np.zeros(self.Bs[i].shape))</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(xs)):</span><br><span class=\"line\">            x = self.model(xs[i:i+<span class=\"number\">1</span>].T)  <span class=\"comment\"># (x, 1)</span></span><br><span class=\"line\">            y = ys[i:i+<span class=\"number\">1</span>].T  <span class=\"comment\"># (x, 1)</span></span><br><span class=\"line\">            index = <span class=\"number\">-1</span></span><br><span class=\"line\">            jacobi = d_loss_f(x[index], y).dot(self.d_activations[index](x[index]))  <span class=\"comment\"># (1, x)*(x, x)</span></span><br><span class=\"line\">            <span class=\"keyword\">while</span> <span class=\"literal\">True</span>:</span><br><span class=\"line\">                grad_w[index] += jacobi.T.dot(x[index<span class=\"number\">-1</span>].T)  <span class=\"comment\"># (x, 1)*(1, y)</span></span><br><span class=\"line\">                grad_b[index] += jacobi.T  <span class=\"comment\"># (x, 1)</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> index == -len(self.Ws):</span><br><span class=\"line\">                    <span class=\"keyword\">break</span></span><br><span class=\"line\">                jacobi = jacobi.dot(self.Ws[index]).dot(self.d_activations[index<span class=\"number\">-1</span>](x[index<span class=\"number\">-1</span>]))  <span class=\"comment\"># (1, x)*(x, y)*(y, y)</span></span><br><span class=\"line\">                index -= <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(self.Ws)):</span><br><span class=\"line\">            grad_w[i] /= len(xs)</span><br><span class=\"line\">            grad_b[i] /= len(xs)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> grad_w, grad_b</span><br></pre></td></tr></table></figure>\n\n<p>Generate non-linear separable dataset.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generate_data</span><span class=\"params\">(n, p=<span class=\"number\">0.8</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># p: percentage of data for training</span></span><br><span class=\"line\">    x11 = np.random.multivariate_normal([<span class=\"number\">4.</span>, <span class=\"number\">3.</span>], [[<span class=\"number\">4.</span>, <span class=\"number\">0.</span>], [<span class=\"number\">0.</span>, <span class=\"number\">1.</span>]], int(<span class=\"number\">0.5</span>*n))</span><br><span class=\"line\">    x12 = np.random.multivariate_normal([<span class=\"number\">2.</span>, <span class=\"number\">-2.</span>], [[<span class=\"number\">1.</span>, <span class=\"number\">0.</span>], [<span class=\"number\">0.</span>, <span class=\"number\">2.</span>]], int(<span class=\"number\">0.25</span>*n))</span><br><span class=\"line\">    x13 = np.random.multivariate_normal([<span class=\"number\">7.</span>, <span class=\"number\">-4.</span>], [[<span class=\"number\">1.</span>, <span class=\"number\">0.</span>], [<span class=\"number\">0.</span>, <span class=\"number\">1.</span>]], int(<span class=\"number\">0.25</span> * n))</span><br><span class=\"line\">    x1 = np.vstack((x11, x12, x13))</span><br><span class=\"line\">    plt.scatter(x1.T[<span class=\"number\">0</span>], x1.T[<span class=\"number\">1</span>], color=<span class=\"string\">\"red\"</span>)</span><br><span class=\"line\">    x2 = np.random.multivariate_normal([<span class=\"number\">6.</span>, <span class=\"number\">0.</span>], [[<span class=\"number\">1.5</span>, <span class=\"number\">0.5</span>], [<span class=\"number\">0.5</span>, <span class=\"number\">1.5</span>]], n)</span><br><span class=\"line\">    plt.scatter(x2.T[<span class=\"number\">0</span>], x2.T[<span class=\"number\">1</span>], color=<span class=\"string\">\"blue\"</span>)</span><br><span class=\"line\">    <span class=\"comment\"># combine data</span></span><br><span class=\"line\">    x = np.vstack((x1, x2))</span><br><span class=\"line\">    y = np.asarray([[<span class=\"number\">1.</span>, <span class=\"number\">0.</span>]] * n + [[<span class=\"number\">0.</span>, <span class=\"number\">1.</span>]] * n)</span><br><span class=\"line\">    <span class=\"comment\"># shuffle data</span></span><br><span class=\"line\">    shuffle_idx = np.arange(<span class=\"number\">0</span>, n*<span class=\"number\">2</span>)</span><br><span class=\"line\">    np.random.shuffle(shuffle_idx)</span><br><span class=\"line\">    x_shuffled = x[shuffle_idx]</span><br><span class=\"line\">    y_shuffled = y[shuffle_idx]</span><br><span class=\"line\">    <span class=\"comment\"># split data into training and testing</span></span><br><span class=\"line\">    _x_train = x_shuffled[<span class=\"number\">0</span>:int(n * p)*<span class=\"number\">2</span>]</span><br><span class=\"line\">    _y_train = y_shuffled[<span class=\"number\">0</span>:int(n * p)*<span class=\"number\">2</span>]</span><br><span class=\"line\">    _x_test = x_shuffled[int(n * p)*<span class=\"number\">2</span>:n*<span class=\"number\">2</span>]</span><br><span class=\"line\">    _y_test = y_shuffled[int(n * p)*<span class=\"number\">2</span>:n*<span class=\"number\">2</span>]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> _x_train, _y_train, _x_test, _y_test</span><br></pre></td></tr></table></figure>\n\n<p>The architecture of neural network two hidden layers with 50 and 5 neurons. Sigmoid activation function for two hidden layers, and soft-max for the output layer:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sigmoid</span><span class=\"params\">(_x)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1.</span> / (<span class=\"number\">1.</span> + np.exp(-_x))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">softmax</span><span class=\"params\">(_x)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> np.exp(_x) / np.sum(np.exp(_x))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">model</span><span class=\"params\">(_x, _theta)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    model architecture:</span></span><br><span class=\"line\"><span class=\"string\">    input layer: m0 = 2</span></span><br><span class=\"line\"><span class=\"string\">        theta[\"w1\"]: (50, 2), theta[\"b1\"]: (50, 1)</span></span><br><span class=\"line\"><span class=\"string\">    hidden layer: m1 = 50, activation: sigmoid</span></span><br><span class=\"line\"><span class=\"string\">        theta[\"w2\"]: (5, 50), theta[\"b2\"]: (5, 1)</span></span><br><span class=\"line\"><span class=\"string\">    hidden layer: m2 = 5, activation: sigmoid</span></span><br><span class=\"line\"><span class=\"string\">        theta[\"w3\"]: (2, 5), theta[\"b3\"]: (2, 1)</span></span><br><span class=\"line\"><span class=\"string\">    output layer: m3 = 2, activation: soft-max</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    x0 = np.array(_x).reshape([<span class=\"number\">2</span>, <span class=\"number\">1</span>])</span><br><span class=\"line\">    x1 = sigmoid(np.dot(_theta[<span class=\"string\">\"w1\"</span>], x0) + _theta[<span class=\"string\">\"b1\"</span>])</span><br><span class=\"line\">    x2 = sigmoid(np.dot(_theta[<span class=\"string\">\"w2\"</span>], x1) + _theta[<span class=\"string\">\"b2\"</span>])</span><br><span class=\"line\">    x3 = softmax(np.dot(_theta[<span class=\"string\">\"w3\"</span>], x2) + _theta[<span class=\"string\">\"b3\"</span>])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> x0, x1, x2, x3</span><br></pre></td></tr></table></figure>\n\n<p>Back propagation for calculating the gradient vector:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">gradient</span><span class=\"params\">(_x, _y, _theta, _model)</span>:</span></span><br><span class=\"line\">    grad = &#123;<span class=\"string\">\"w1\"</span>: np.zeros([<span class=\"number\">50</span>, <span class=\"number\">2</span>]), <span class=\"string\">\"b1\"</span>: np.zeros([<span class=\"number\">50</span>, <span class=\"number\">1</span>])</span><br><span class=\"line\">            , <span class=\"string\">\"w2\"</span>: np.zeros([<span class=\"number\">5</span>, <span class=\"number\">50</span>]), <span class=\"string\">\"b2\"</span>: np.zeros([<span class=\"number\">5</span>, <span class=\"number\">1</span>])</span><br><span class=\"line\">            , <span class=\"string\">\"w3\"</span>: np.zeros([<span class=\"number\">2</span>, <span class=\"number\">5</span>]), <span class=\"string\">\"b3\"</span>: np.zeros([<span class=\"number\">2</span>, <span class=\"number\">1</span>])&#125;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(_x)):</span><br><span class=\"line\">        x0, x1, x2, x3 = model(_x[i].reshape([<span class=\"number\">2</span>, <span class=\"number\">1</span>]), _theta)</span><br><span class=\"line\">        <span class=\"comment\"># back propagation</span></span><br><span class=\"line\">        _loss_to_x3 = (-_y[i].reshape([<span class=\"number\">2</span>, <span class=\"number\">1</span>]) / x3).T</span><br><span class=\"line\">        _x3_to_a3 = np.diag(x3.reshape([<span class=\"number\">2</span>, ])) - x3.dot(x3.T)</span><br><span class=\"line\">        _loss_to_a3 = _loss_to_x3.dot(_x3_to_a3)</span><br><span class=\"line\">        grad[<span class=\"string\">\"w3\"</span>] += np.dot(_loss_to_a3.T, x2.T)</span><br><span class=\"line\">        grad[<span class=\"string\">\"b3\"</span>] += _loss_to_a3.T</span><br><span class=\"line\">        _a3_to_x2 = _theta[<span class=\"string\">\"w3\"</span>]</span><br><span class=\"line\">        _x2_to_a2 = np.diag((x2 - x2 * x2).reshape([<span class=\"number\">5</span>, ]))</span><br><span class=\"line\">        _loss_to_a2 = _loss_to_a3.dot(_a3_to_x2).dot(_x2_to_a2)</span><br><span class=\"line\">        grad[<span class=\"string\">\"w2\"</span>] += np.dot(_loss_to_a2.T, x1.T)</span><br><span class=\"line\">        grad[<span class=\"string\">\"b2\"</span>] += _loss_to_a2.T</span><br><span class=\"line\">        _a2_to_x1 = _theta[<span class=\"string\">\"w2\"</span>]</span><br><span class=\"line\">        _x1_to_a1 = np.diag((x1 - x1 * x1).reshape([<span class=\"number\">50</span>, ]))</span><br><span class=\"line\">        _loss_to_a1 = _loss_to_a2.dot(_a2_to_x1).dot(_x1_to_a1)</span><br><span class=\"line\">        grad[<span class=\"string\">\"w1\"</span>] += np.dot(_loss_to_a1.T, x0.T)</span><br><span class=\"line\">        grad[<span class=\"string\">\"b1\"</span>] += _loss_to_a1.T</span><br><span class=\"line\">    <span class=\"keyword\">return</span> grad</span><br></pre></td></tr></table></figure>\n\n<p>Result:</p>\n<p><img src=\"https://github.com/dbddqy/Note/raw/master/Deep_Learning/exercise_dnn_classification/result.png\" alt=\"\"></p>\n","categories":["exercise"],"tags":["deep learning"]},{"title":"[Exercise] Neural Network from scratch - Classification","url":"https://dbddqy.github.io/2019/06/26/e-nn_classification/","content":"<h1 id=\"Neural-Network-from-scratch-Classification\"><a href=\"#Neural-Network-from-scratch-Classification\" class=\"headerlink\" title=\"Neural Network from scratch - Classification\"></a>Neural Network from scratch - Classification</h1><p><a href=\"https://github.com/dbddqy/Note/blob/master/Deep_Learning/exercise_nn_classification/nn_classification.py\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/Github-Source%20Code-blue\" alt=\"\"></a></p>\n<p>Generate non-linear separable dataset.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generate_data</span><span class=\"params\">(n, p=<span class=\"number\">0.8</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># p: percentage of data for training</span></span><br><span class=\"line\">    x11 = np.random.multivariate_normal([<span class=\"number\">4.</span>, <span class=\"number\">3.</span>], [[<span class=\"number\">4.</span>, <span class=\"number\">0.</span>], [<span class=\"number\">0.</span>, <span class=\"number\">1.</span>]], int(<span class=\"number\">0.5</span>*n))</span><br><span class=\"line\">    x12 = np.random.multivariate_normal([<span class=\"number\">2.</span>, <span class=\"number\">-2.</span>], [[<span class=\"number\">1.</span>, <span class=\"number\">0.</span>], [<span class=\"number\">0.</span>, <span class=\"number\">2.</span>]], int(<span class=\"number\">0.25</span>*n))</span><br><span class=\"line\">    x13 = np.random.multivariate_normal([<span class=\"number\">7.</span>, <span class=\"number\">-4.</span>], [[<span class=\"number\">1.</span>, <span class=\"number\">0.</span>], [<span class=\"number\">0.</span>, <span class=\"number\">1.</span>]], int(<span class=\"number\">0.25</span> * n))</span><br><span class=\"line\">    x1 = np.vstack((x11, x12, x13))</span><br><span class=\"line\">    plt.scatter(x1.T[<span class=\"number\">0</span>], x1.T[<span class=\"number\">1</span>], color=<span class=\"string\">\"red\"</span>)</span><br><span class=\"line\">    x2 = np.random.multivariate_normal([<span class=\"number\">6.</span>, <span class=\"number\">0.</span>], [[<span class=\"number\">1.5</span>, <span class=\"number\">0.5</span>], [<span class=\"number\">0.5</span>, <span class=\"number\">1.5</span>]], n)</span><br><span class=\"line\">    plt.scatter(x2.T[<span class=\"number\">0</span>], x2.T[<span class=\"number\">1</span>], color=<span class=\"string\">\"blue\"</span>)</span><br><span class=\"line\">    <span class=\"comment\"># combine data</span></span><br><span class=\"line\">    x = np.vstack((x1, x2))</span><br><span class=\"line\">    y = np.asarray([[<span class=\"number\">1.</span>, <span class=\"number\">0.</span>]] * n + [[<span class=\"number\">0.</span>, <span class=\"number\">1.</span>]] * n)</span><br><span class=\"line\">    <span class=\"comment\"># shuffle data</span></span><br><span class=\"line\">    shuffle_idx = np.arange(<span class=\"number\">0</span>, n*<span class=\"number\">2</span>)</span><br><span class=\"line\">    np.random.shuffle(shuffle_idx)</span><br><span class=\"line\">    x_shuffled = x[shuffle_idx]</span><br><span class=\"line\">    y_shuffled = y[shuffle_idx]</span><br><span class=\"line\">    <span class=\"comment\"># split data into training and testing</span></span><br><span class=\"line\">    _x_train = x_shuffled[<span class=\"number\">0</span>:int(n * p)*<span class=\"number\">2</span>]</span><br><span class=\"line\">    _y_train = y_shuffled[<span class=\"number\">0</span>:int(n * p)*<span class=\"number\">2</span>]</span><br><span class=\"line\">    _x_test = x_shuffled[int(n * p)*<span class=\"number\">2</span>:n*<span class=\"number\">2</span>]</span><br><span class=\"line\">    _y_test = y_shuffled[int(n * p)*<span class=\"number\">2</span>:n*<span class=\"number\">2</span>]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> _x_train, _y_train, _x_test, _y_test</span><br></pre></td></tr></table></figure>\n\n<p>The architecture of neural network two hidden layers with 50 and 5 neurons. Sigmoid activation function for two hidden layers, and soft-max for the output layer:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sigmoid</span><span class=\"params\">(_x)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1.</span> / (<span class=\"number\">1.</span> + np.exp(-_x))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">softmax</span><span class=\"params\">(_x)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> np.exp(_x) / np.sum(np.exp(_x))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">model</span><span class=\"params\">(_x, _theta)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    model architecture:</span></span><br><span class=\"line\"><span class=\"string\">    input layer: m0 = 2</span></span><br><span class=\"line\"><span class=\"string\">        theta[\"w1\"]: (50, 2), theta[\"b1\"]: (50, 1)</span></span><br><span class=\"line\"><span class=\"string\">    hidden layer: m1 = 50, activation: sigmoid</span></span><br><span class=\"line\"><span class=\"string\">        theta[\"w2\"]: (5, 50), theta[\"b2\"]: (5, 1)</span></span><br><span class=\"line\"><span class=\"string\">    hidden layer: m2 = 5, activation: sigmoid</span></span><br><span class=\"line\"><span class=\"string\">        theta[\"w3\"]: (2, 5), theta[\"b3\"]: (2, 1)</span></span><br><span class=\"line\"><span class=\"string\">    output layer: m3 = 2, activation: soft-max</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    x0 = np.array(_x).reshape([<span class=\"number\">2</span>, <span class=\"number\">1</span>])</span><br><span class=\"line\">    x1 = sigmoid(np.dot(_theta[<span class=\"string\">\"w1\"</span>], x0) + _theta[<span class=\"string\">\"b1\"</span>])</span><br><span class=\"line\">    x2 = sigmoid(np.dot(_theta[<span class=\"string\">\"w2\"</span>], x1) + _theta[<span class=\"string\">\"b2\"</span>])</span><br><span class=\"line\">    x3 = softmax(np.dot(_theta[<span class=\"string\">\"w3\"</span>], x2) + _theta[<span class=\"string\">\"b3\"</span>])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> x0, x1, x2, x3</span><br></pre></td></tr></table></figure>\n\n<p>Back propagation for calculating the gradient vector:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">gradient</span><span class=\"params\">(_x, _y, _theta, _model)</span>:</span></span><br><span class=\"line\">    grad = &#123;<span class=\"string\">\"w1\"</span>: np.zeros([<span class=\"number\">50</span>, <span class=\"number\">2</span>]), <span class=\"string\">\"b1\"</span>: np.zeros([<span class=\"number\">50</span>, <span class=\"number\">1</span>])</span><br><span class=\"line\">            , <span class=\"string\">\"w2\"</span>: np.zeros([<span class=\"number\">5</span>, <span class=\"number\">50</span>]), <span class=\"string\">\"b2\"</span>: np.zeros([<span class=\"number\">5</span>, <span class=\"number\">1</span>])</span><br><span class=\"line\">            , <span class=\"string\">\"w3\"</span>: np.zeros([<span class=\"number\">2</span>, <span class=\"number\">5</span>]), <span class=\"string\">\"b3\"</span>: np.zeros([<span class=\"number\">2</span>, <span class=\"number\">1</span>])&#125;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(_x)):</span><br><span class=\"line\">        x0, x1, x2, x3 = model(_x[i].reshape([<span class=\"number\">2</span>, <span class=\"number\">1</span>]), _theta)</span><br><span class=\"line\">        <span class=\"comment\"># back propagation</span></span><br><span class=\"line\">        _loss_to_x3 = (-_y[i].reshape([<span class=\"number\">2</span>, <span class=\"number\">1</span>]) / x3).T</span><br><span class=\"line\">        _x3_to_a3 = np.diag(x3.reshape([<span class=\"number\">2</span>, ])) - x3.dot(x3.T)</span><br><span class=\"line\">        _loss_to_a3 = _loss_to_x3.dot(_x3_to_a3)</span><br><span class=\"line\">        grad[<span class=\"string\">\"w3\"</span>] += np.dot(_loss_to_a3.T, x2.T)</span><br><span class=\"line\">        grad[<span class=\"string\">\"b3\"</span>] += _loss_to_a3.T</span><br><span class=\"line\">        _a3_to_x2 = _theta[<span class=\"string\">\"w3\"</span>]</span><br><span class=\"line\">        _x2_to_a2 = np.diag((x2 - x2 * x2).reshape([<span class=\"number\">5</span>, ]))</span><br><span class=\"line\">        _loss_to_a2 = _loss_to_a3.dot(_a3_to_x2).dot(_x2_to_a2)</span><br><span class=\"line\">        grad[<span class=\"string\">\"w2\"</span>] += np.dot(_loss_to_a2.T, x1.T)</span><br><span class=\"line\">        grad[<span class=\"string\">\"b2\"</span>] += _loss_to_a2.T</span><br><span class=\"line\">        _a2_to_x1 = _theta[<span class=\"string\">\"w2\"</span>]</span><br><span class=\"line\">        _x1_to_a1 = np.diag((x1 - x1 * x1).reshape([<span class=\"number\">50</span>, ]))</span><br><span class=\"line\">        _loss_to_a1 = _loss_to_a2.dot(_a2_to_x1).dot(_x1_to_a1)</span><br><span class=\"line\">        grad[<span class=\"string\">\"w1\"</span>] += np.dot(_loss_to_a1.T, x0.T)</span><br><span class=\"line\">        grad[<span class=\"string\">\"b1\"</span>] += _loss_to_a1.T</span><br><span class=\"line\">    <span class=\"keyword\">return</span> grad</span><br></pre></td></tr></table></figure>\n\n<p>Result:</p>\n<p><img src=\"https://github.com/dbddqy/Note/raw/master/Deep_Learning/exercise_nn_classification/result.png\" alt=\"\"></p>\n","categories":["exercise"],"tags":["deep learning"]},{"title":"[Exercise] Neural Network from scratch - Regression","url":"https://dbddqy.github.io/2019/06/25/e-nn_regression/","content":"<h1 id=\"Neural-Network-from-scratch-Regression\"><a href=\"#Neural-Network-from-scratch-Regression\" class=\"headerlink\" title=\"Neural Network from scratch - Regression\"></a>Neural Network from scratch - Regression</h1><p><a href=\"https://github.com/dbddqy/Note/blob/master/Deep_Learning/exercise_nn_regression/nn_regression.py\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/Github-Source%20Code-blue\" alt=\"\"></a></p>\n<p>Generate data from y = sin(x^2 +1) + noise:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generate_data</span><span class=\"params\">(n, low=<span class=\"number\">0.0</span>, high=<span class=\"number\">3.0</span>)</span>:</span></span><br><span class=\"line\">    step = (high - low) / n</span><br><span class=\"line\">    x, y = [], []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(n):</span><br><span class=\"line\">        x.append(low + step * i)</span><br><span class=\"line\">        y.append(sin(x[i] * x[i] + <span class=\"number\">1.</span>) + np.random.normal(<span class=\"number\">0.0</span>, <span class=\"number\">0.1</span>))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> np.array(x), np.array(y)</span><br></pre></td></tr></table></figure>\n\n<p>The architecture of neural network two hidden layers with 50 and 5 neurons and sigmoid activation function:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sigmoid</span><span class=\"params\">(_x)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1.</span> / (<span class=\"number\">1.</span> + np.exp(-_x))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">model</span><span class=\"params\">(_x, _theta)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    model architecture:</span></span><br><span class=\"line\"><span class=\"string\">    input layer: m0 = 1</span></span><br><span class=\"line\"><span class=\"string\">        theta[\"w1\"]: (50, 1), theta[\"b1\"]: (50, 1)</span></span><br><span class=\"line\"><span class=\"string\">    hidden layer: m1 = 50, activation: sigmoid</span></span><br><span class=\"line\"><span class=\"string\">        theta[\"w2\"]: (5, 50), theta[\"b2\"]: (5, 1)</span></span><br><span class=\"line\"><span class=\"string\">    hidden layer: m2 = 5, activation: sigmoid</span></span><br><span class=\"line\"><span class=\"string\">        theta[\"w3\"]: (1, 5), theta[\"b3\"]: (1, 1)</span></span><br><span class=\"line\"><span class=\"string\">    output layer: m3 = 1</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    x0 = np.array(_x).reshape([<span class=\"number\">1</span>, <span class=\"number\">1</span>])</span><br><span class=\"line\">    x1 = sigmoid(np.dot(_theta[<span class=\"string\">\"w1\"</span>], x0) + _theta[<span class=\"string\">\"b1\"</span>])</span><br><span class=\"line\">    x2 = sigmoid(np.dot(_theta[<span class=\"string\">\"w2\"</span>], x1) + _theta[<span class=\"string\">\"b2\"</span>])</span><br><span class=\"line\">    x3 = np.dot(_theta[<span class=\"string\">\"w3\"</span>], x2) + _theta[<span class=\"string\">\"b3\"</span>]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> x0, x1, x2, x3</span><br></pre></td></tr></table></figure>\n\n<p>Back propagation for calculating the gradient vector:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">gradient</span><span class=\"params\">(_x, _y, _theta, _model)</span>:</span></span><br><span class=\"line\">    grad = &#123;<span class=\"string\">\"w1\"</span>: np.zeros((<span class=\"number\">50</span>, <span class=\"number\">1</span>)), <span class=\"string\">\"b1\"</span>: np.zeros((<span class=\"number\">50</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">            , <span class=\"string\">\"w2\"</span>: np.zeros((<span class=\"number\">5</span>, <span class=\"number\">50</span>)), <span class=\"string\">\"b2\"</span>: np.zeros((<span class=\"number\">5</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">            , <span class=\"string\">\"w3\"</span>: np.zeros((<span class=\"number\">1</span>, <span class=\"number\">5</span>)), <span class=\"string\">\"b3\"</span>: np.zeros((<span class=\"number\">1</span>, <span class=\"number\">1</span>))&#125;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(_x)):</span><br><span class=\"line\">        x0, x1, x2, x3 = model(_x[i], _theta)</span><br><span class=\"line\">        <span class=\"comment\"># back propagation</span></span><br><span class=\"line\">        _loss_to_x3 = <span class=\"number\">-2</span> * (np.array(_y[i]).reshape([<span class=\"number\">1</span>, <span class=\"number\">1</span>]) - x3)</span><br><span class=\"line\">        grad[<span class=\"string\">\"w3\"</span>] += np.dot(_loss_to_x3.T, x2.T)</span><br><span class=\"line\">        grad[<span class=\"string\">\"b3\"</span>] += _loss_to_x3.T</span><br><span class=\"line\">        _x3_to_x2 = _theta[<span class=\"string\">\"w3\"</span>]</span><br><span class=\"line\">        _x2_to_a2 = np.diag((x2 - x2 * x2).reshape([<span class=\"number\">5</span>, ]))</span><br><span class=\"line\">        _loss_to_a2 = _loss_to_x3.dot(_x3_to_x2).dot(_x2_to_a2)</span><br><span class=\"line\">        grad[<span class=\"string\">\"w2\"</span>] += np.dot(_loss_to_a2.T, x1.T)</span><br><span class=\"line\">        grad[<span class=\"string\">\"b2\"</span>] += _loss_to_a2.T</span><br><span class=\"line\">        _a2_to_x1 = _theta[<span class=\"string\">\"w2\"</span>]</span><br><span class=\"line\">        _x1_to_a1 = np.diag((x1 - x1 * x1).reshape([<span class=\"number\">50</span>, ]))</span><br><span class=\"line\">        _loss_to_a1 = _loss_to_a2.dot(_a2_to_x1).dot(_x1_to_a1)</span><br><span class=\"line\">        grad[<span class=\"string\">\"w1\"</span>] += np.dot(_loss_to_a1.T, x0.T)</span><br><span class=\"line\">        grad[<span class=\"string\">\"b1\"</span>] += _loss_to_a1.T</span><br><span class=\"line\">    <span class=\"keyword\">return</span> grad</span><br></pre></td></tr></table></figure>\n\n<p>Resutlt:</p>\n<p><img src=\"https://github.com/dbddqy/Note/raw/master/Deep_Learning/exercise_nn_regression/result.png\" alt=\"\"></p>\n","categories":["exercise"],"tags":["deep learning"]},{"title":"[Exercise] Logistic Regression","url":"https://dbddqy.github.io/2019/06/23/e-logistic_regression/","content":"<h1 id=\"Logistic-Regression\"><a href=\"#Logistic-Regression\" class=\"headerlink\" title=\"Logistic Regression\"></a>Logistic Regression</h1><p><a href=\"https://github.com/dbddqy/Note/blob/master/Deep_Learning/exercise_logistic_regression/logistic_regression.py\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/Github-Source%20Code-blue\" alt=\"\"></a></p>\n<p>Generate training and testing date set:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generate_data</span><span class=\"params\">(n, p=<span class=\"number\">0.8</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># p: percentage of data for training</span></span><br><span class=\"line\">    x1 = np.random.multivariate_normal([<span class=\"number\">4.</span>, <span class=\"number\">3.</span>], [[<span class=\"number\">4.</span>, <span class=\"number\">0.</span>], [<span class=\"number\">0.</span>, <span class=\"number\">1.</span>]], n)</span><br><span class=\"line\">    plt.scatter(x1.T[<span class=\"number\">0</span>], x1.T[<span class=\"number\">1</span>], color=<span class=\"string\">\"red\"</span>)</span><br><span class=\"line\">    x2 = np.random.multivariate_normal([<span class=\"number\">6.</span>, <span class=\"number\">0.</span>], [[<span class=\"number\">1.5</span>, <span class=\"number\">0.5</span>], [<span class=\"number\">0.5</span>, <span class=\"number\">1.5</span>]], n)</span><br><span class=\"line\">    plt.scatter(x2.T[<span class=\"number\">0</span>], x2.T[<span class=\"number\">1</span>], color=<span class=\"string\">\"blue\"</span>)</span><br><span class=\"line\">    <span class=\"comment\"># combine data</span></span><br><span class=\"line\">    x = np.vstack((x1, x2))</span><br><span class=\"line\">    y = np.asarray([[<span class=\"number\">1.</span>, <span class=\"number\">0.</span>]] * n + [[<span class=\"number\">0.</span>, <span class=\"number\">1.</span>]] * n)</span><br><span class=\"line\">    <span class=\"comment\"># shuffle data</span></span><br><span class=\"line\">    shuffle_idx = np.arange(<span class=\"number\">0</span>, n*<span class=\"number\">2</span>)</span><br><span class=\"line\">    np.random.shuffle(shuffle_idx)</span><br><span class=\"line\">    x_shuffled = x[shuffle_idx]</span><br><span class=\"line\">    y_shuffled = y[shuffle_idx]</span><br><span class=\"line\">    <span class=\"comment\"># split data into training and testing</span></span><br><span class=\"line\">    _x_train = x_shuffled[<span class=\"number\">0</span>:int(n * p)*<span class=\"number\">2</span>]</span><br><span class=\"line\">    _y_train = y_shuffled[<span class=\"number\">0</span>:int(n * p)*<span class=\"number\">2</span>]</span><br><span class=\"line\">    _x_test = x_shuffled[int(n * p)*<span class=\"number\">2</span>:n*<span class=\"number\">2</span>]</span><br><span class=\"line\">    _y_test = y_shuffled[int(n * p)*<span class=\"number\">2</span>:n*<span class=\"number\">2</span>]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> _x_train, _y_train, _x_test, _y_test</span><br></pre></td></tr></table></figure>\n\n<p>Model of logistic regression:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">model</span><span class=\"params\">(_x, _theta)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1.</span> / (<span class=\"number\">1.</span> + exp(-_x[<span class=\"number\">0</span>] * _theta[<span class=\"number\">0</span>] - _x[<span class=\"number\">1</span>] * _theta[<span class=\"number\">1</span>] - _theta[<span class=\"number\">2</span>]))</span><br></pre></td></tr></table></figure>\n\n<p>Calculating the gradient vector:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">gradient</span><span class=\"params\">(_x, _y, _theta, _model)</span>:</span></span><br><span class=\"line\">    grad = np.array([<span class=\"number\">0.</span>, <span class=\"number\">0.</span>, <span class=\"number\">0.</span>])</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(_x)):</span><br><span class=\"line\">        grad[<span class=\"number\">0</span>] += (-(<span class=\"number\">1.</span> - _model(_x[i], _theta)) * _y[i][<span class=\"number\">0</span>] + _model(_x[i], _theta) * _y[i][<span class=\"number\">1</span>]) * _x[i][<span class=\"number\">0</span>]</span><br><span class=\"line\">        grad[<span class=\"number\">1</span>] += (-(<span class=\"number\">1.</span> - _model(_x[i], _theta)) * _y[i][<span class=\"number\">0</span>] + _model(_x[i], _theta) * _y[i][<span class=\"number\">1</span>]) * _x[i][<span class=\"number\">1</span>]</span><br><span class=\"line\">        grad[<span class=\"number\">2</span>] += (-(<span class=\"number\">1.</span> - _model(_x[i], _theta)) * _y[i][<span class=\"number\">0</span>] + _model(_x[i], _theta) * _y[i][<span class=\"number\">1</span>])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> grad</span><br></pre></td></tr></table></figure>\n\n<p>Result:</p>\n<blockquote>\n<p>step: 0, theta: 0.951656 1.001376 -0.007945<br>training error rate: 0.506250, testing error rate: 0.450000<br>step: 100, theta: -0.314369 1.131259 -0.194332<br>training error rate: 0.043750, testing error rate: 0.175000<br>step: 200, theta: -0.361768 1.302411 -0.186887<br>training error rate: 0.043750, testing error rate: 0.150000<br>step: 300, theta: -0.395465 1.433055 -0.181936<br>training error rate: 0.043750, testing error rate: 0.150000<br>step: 400, theta: -0.422276 1.538629 -0.178686<br>training error rate: 0.043750, testing error rate: 0.150000<br>step: 500, theta: -0.444488 1.627198 -0.176557<br>training error rate: 0.050000, testing error rate: 0.150000<br>step: 600, theta: -0.463398 1.703425 -0.175217<br>training error rate: 0.050000, testing error rate: 0.150000<br>step: 700, theta: -0.479820 1.770257 -0.174460<br>training error rate: 0.050000, testing error rate: 0.150000<br>step: 800, theta: -0.494296 1.829684 -0.174151<br>training error rate: 0.050000, testing error rate: 0.150000<br>step: 900, theta: -0.507206 1.883114 -0.174194<br>training error rate: 0.050000, testing error rate: 0.150000</p>\n</blockquote>\n<p><img src=\"https://github.com/dbddqy/Note/raw/master/Deep_Learning/exercise_logistic_regression/result.png\" alt=\"\"></p>\n","categories":["exercise"],"tags":["deep learning"]},{"title":"[Exercise] Linear Regression","url":"https://dbddqy.github.io/2019/06/22/e-linear_regression/","content":"<h1 id=\"Linear-Regression\"><a href=\"#Linear-Regression\" class=\"headerlink\" title=\"Linear Regression\"></a>Linear Regression</h1><p><a href=\"https://github.com/dbddqy/Note/blob/master/Deep_Learning/exercise_linear_regression/linear_regression.py\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/Github-Source%20Code-blue\" alt=\"\"></a></p>\n<p>Data generated from function y = 2*x + 0.5 + noise:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generate_data</span><span class=\"params\">(n, k, b, low=<span class=\"number\">0.0</span>, high=<span class=\"number\">10.0</span>)</span>:</span></span><br><span class=\"line\">    step = (high - low) / n</span><br><span class=\"line\">    x, y = [], []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(n):</span><br><span class=\"line\">        x.append(low + step * i)</span><br><span class=\"line\">        y.append(x[i] * k + b + np.random.normal(<span class=\"number\">0.0</span>, <span class=\"number\">0.05</span> * (high - low)))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> np.array(x), np.array(y)</span><br><span class=\"line\"></span><br><span class=\"line\">x_train, y_train = generate_data(<span class=\"number\">100</span>, <span class=\"number\">2.0</span>, <span class=\"number\">0.5</span>)</span><br></pre></td></tr></table></figure>\n\n<p>The linear model:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">model</span><span class=\"params\">(_x, _theta)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> _x * _theta[<span class=\"number\">0</span>] + _theta[<span class=\"number\">1</span>]</span><br></pre></td></tr></table></figure>\n\n<p>Gradient function:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">gradient</span><span class=\"params\">(_x, _y, _theta, _model)</span>:</span></span><br><span class=\"line\">    g = np.array([<span class=\"number\">0.0</span>, <span class=\"number\">0.0</span>])</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(_x)):</span><br><span class=\"line\">        g[<span class=\"number\">0</span>] += (model(_x[i], _theta) - _y[i]) * _x[i]</span><br><span class=\"line\">        g[<span class=\"number\">1</span>] += (model(_x[i], _theta) - _y[i])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> g</span><br></pre></td></tr></table></figure>\n\n<p>Training with gradient descent:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train</span><span class=\"params\">(_x, _y, _theta, _model, lr=<span class=\"number\">1e-5</span>, steps=<span class=\"number\">1000</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> step <span class=\"keyword\">in</span> range(steps):</span><br><span class=\"line\">        _theta -= lr * gradient(_x, _y, _theta, _model)</span><br><span class=\"line\">        <span class=\"comment\"># print log</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> step % <span class=\"number\">100</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            print(<span class=\"string\">\"step: %d, theta: %f %f, loss: %f\"</span> % (step, _theta[<span class=\"number\">0</span>], _theta[<span class=\"number\">1</span>], loss(_x, _y, _theta, _model)))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> _theta</span><br></pre></td></tr></table></figure>\n\n<p>Result:<br><img src=\"https://github.com/dbddqy/Note/raw/master/Deep_Learning/exercise_linear_regression/result.png\" alt=\"\"></p>\n","categories":["exercise"],"tags":["deep learning"]},{"title":"[Theory] [Deep Learning] ch9","url":"https://dbddqy.github.io/2019/06/19/t-deep_learning_ch9/","content":"<h1 id=\"Unsupervised-Learning-and-Generative-Models\"><a href=\"#Unsupervised-Learning-and-Generative-Models\" class=\"headerlink\" title=\"Unsupervised Learning and Generative Models\"></a>Unsupervised Learning and Generative Models</h1><h2 id=\"Auto-encoder\"><a href=\"#Auto-encoder\" class=\"headerlink\" title=\"Auto-encoder\"></a>Auto-encoder</h2><ul>\n<li>latent info extraction</li>\n<li>dimension reduction</li>\n</ul>\n<h2 id=\"Variational-Auto-encoder\"><a href=\"#Variational-Auto-encoder\" class=\"headerlink\" title=\"Variational Auto-encoder\"></a>Variational Auto-encoder</h2><ul>\n<li>generative</li>\n<li>the latent variable should follow a known distribution</li>\n<li>after training, use decoder only to generate new sample</li>\n</ul>\n<h2 id=\"GAN\"><a href=\"#GAN\" class=\"headerlink\" title=\"GAN\"></a>GAN</h2><ul>\n<li>use a Generator and a Discriminator network</li>\n<li>mini-max optimization: difficult to train</li>\n</ul>\n","categories":["theory"],"tags":["deep learning"]},{"title":"[Theory] [Deep Learning] ch7","url":"https://dbddqy.github.io/2019/05/22/t-deep_learning_ch7/","content":"<h1 id=\"CNN\"><a href=\"#CNN\" class=\"headerlink\" title=\"CNN\"></a>CNN</h1><h2 id=\"Convolutional-Layer\"><a href=\"#Convolutional-Layer\" class=\"headerlink\" title=\"Convolutional Layer\"></a>Convolutional Layer</h2><p><img src=\"https://github.com/dbddqy/Note/raw/master/Deep_Learning/pics/CNN.png\" alt=\"\"></p>\n<p>Filter the data with learnable kernel parameters.</p>\n<h2 id=\"Modifications\"><a href=\"#Modifications\" class=\"headerlink\" title=\"Modifications\"></a>Modifications</h2><ul>\n<li>Padding: p</li>\n<li>Stride: s</li>\n<li>Dilated: d</li>\n</ul>\n<p>Output size:<br>$$<br>[\\frac{M_{L-1}+2p-(K_L-1)d-1}{s}]+1<br>$$</p>\n<h2 id=\"Pooling\"><a href=\"#Pooling\" class=\"headerlink\" title=\"Pooling\"></a>Pooling</h2><ul>\n<li>max pooling</li>\n<li>average pooling</li>\n</ul>\n<p>Reduced spatial dimensions of the feature map. And the operation is translational-invariant.</p>\n","categories":["theory"],"tags":["deep learning"]},{"title":"[Theory] [Deep Learning] ch6","url":"https://dbddqy.github.io/2019/05/11/t-deep_learning_ch6/","content":"<h1 id=\"Model-Capacity-and-Overfitting\"><a href=\"#Model-Capacity-and-Overfitting\" class=\"headerlink\" title=\"Model Capacity and Overfitting\"></a>Model Capacity and Overfitting</h1><h2 id=\"Regularization\"><a href=\"#Regularization\" class=\"headerlink\" title=\"Regularization\"></a>Regularization</h2><p>add penalty terms P(W)<br>$$<br>L_r(\\theta)=L(\\theta)+P(W)<br>$$</p>\n<ul>\n<li><p>L2: prefer smaller weight<br>$$<br>P(W) = ||vector(W)||_2^2<br>$$</p>\n</li>\n<li><p>L1:<br>$$<br>P(W) = ||vector(W)||_1<br>$$</p>\n</li>\n</ul>\n<h2 id=\"Early-Stopping\"><a href=\"#Early-Stopping\" class=\"headerlink\" title=\"Early Stopping\"></a>Early Stopping</h2><p>stop when testing error begins to increase</p>\n<h2 id=\"Data-Augmentation\"><a href=\"#Data-Augmentation\" class=\"headerlink\" title=\"Data Augmentation\"></a>Data Augmentation</h2><p>easy for classification, modify input sample a bit without changing class label</p>\n<h2 id=\"Dropout\"><a href=\"#Dropout\" class=\"headerlink\" title=\"Dropout\"></a>Dropout</h2><p>During each minibatch, randomly set the output of some neurons in layer l to zero.</p>\n<h2 id=\"Hyperparameter-Optimization\"><a href=\"#Hyperparameter-Optimization\" class=\"headerlink\" title=\"Hyperparameter Optimization\"></a>Hyperparameter Optimization</h2><p>Grid search, try and error. Require a validation set.</p>\n","categories":["theory"],"tags":["deep learning"]},{"title":"[Theory] [Deep Learning] ch5","url":"https://dbddqy.github.io/2019/05/04/t-deep_learning_ch5/","content":"<h1 id=\"Training-Techniques\"><a href=\"#Training-Techniques\" class=\"headerlink\" title=\"Training Techniques\"></a>Training Techniques</h1><h2 id=\"Momentum\"><a href=\"#Momentum\" class=\"headerlink\" title=\"Momentum\"></a>Momentum</h2><p>Use gradient part plus momentum part to update parameters<br>$$<br>\\Delta\\theta^t = \\beta\\Delta\\theta^{t-1}-\\gamma^t\\nabla L(t;\\theta)|_{\\theta=\\theta^t}<br>$$<br>$\\beta \\in [0,1)$ is the momentum factor, $\\gamma^t$ is learning rate in t-th iteration.</p>\n<ul>\n<li>reduce noise in Stochastic gradient descent</li>\n<li>reduce oscillation and accelerate convergence for ill conditioned contour lines</li>\n</ul>\n<p>Nesterov momentum:<br>$$<br>\\Delta\\theta^t = \\beta\\Delta\\theta^{t-1}-\\gamma^t\\nabla L(t;\\theta)|_{\\theta=\\theta^t+\\beta\\Delta\\theta^{t-1}}<br>$$</p>\n<h2 id=\"Adaptive-schedules\"><a href=\"#Adaptive-schedules\" class=\"headerlink\" title=\"Adaptive schedules\"></a>Adaptive schedules</h2><ul>\n<li>RMSprop</li>\n<li>Adam</li>\n<li>AdaGrad</li>\n<li>AdaDelta</li>\n<li>…</li>\n</ul>\n<h2 id=\"Batch-Normalization\"><a href=\"#Batch-Normalization\" class=\"headerlink\" title=\"Batch Normalization\"></a>Batch Normalization</h2><p>In layer l, $x_{l-1} \\rightarrow a_{l-1} \\overset{\\phi}{\\rightarrow} x_l$ , add learnable parameter $\\gamma_L$ and $\\beta_L$ , to avoid covariate shift over layers and over time during training</p>\n<img src=\"https://github.com/dbddqy/Note/raw/master/Deep_Learning/pics/batch_nor_1.png\" style=\"zoom:50%;\" />\n\n<img src=\"https://github.com/dbddqy/Note/raw/master/Deep_Learning/pics/batch_nor_2.png\" style=\"zoom:50%;\" />\n\n<h2 id=\"Parameter-Initialization\"><a href=\"#Parameter-Initialization\" class=\"headerlink\" title=\"Parameter Initialization\"></a>Parameter Initialization</h2><ul>\n<li>bias vector: zero</li>\n<li>weight matrices: random or He Initialization</li>\n</ul>\n<h2 id=\"Shortcut\"><a href=\"#Shortcut\" class=\"headerlink\" title=\"Shortcut\"></a>Shortcut</h2><p>skip connection, shortcut, residual network (ResNet)</p>\n<ul>\n<li>forward: make low level feature available in deep layers</li>\n<li>backward: avoid vanishing gradient</li>\n</ul>\n","categories":["theory"],"tags":["deep learning"]},{"title":"[Theory] [Deep Learning] ch4","url":"https://dbddqy.github.io/2019/05/03/t-deep_learning_ch4/","content":"<h1 id=\"Fully-Connected-Neural-Networks\"><a href=\"#Fully-Connected-Neural-Networks\" class=\"headerlink\" title=\"Fully Connected Neural Networks\"></a>Fully Connected Neural Networks</h1><h2 id=\"Model-Architecture\"><a href=\"#Model-Architecture\" class=\"headerlink\" title=\"Model Architecture\"></a>Model Architecture</h2><p><img src=\"https://github.com/dbddqy/Note/raw/master/Deep_Learning/pics/model.png\" alt=\"\"></p>\n<h2 id=\"Activation-Functions\"><a href=\"#Activation-Functions\" class=\"headerlink\" title=\"Activation Functions\"></a>Activation Functions</h2><ol>\n<li>Linear (identity):</li>\n</ol>\n<p>$$<br>\\phi(a) = a<br>$$<br>2. Unit step:</p>\n<p>$$<br>\\phi(a) = u(a) = \\begin{cases}<br>     1 &amp;a\\ge0<br>\\\\ 0 &amp;a&lt;0<br>\\end{cases}<br>$$</p>\n<ol start=\"3\">\n<li>Sign Function:</li>\n</ol>\n<p>$$<br>\\phi(a) = sign(a) = \\begin{cases}<br>     1 &amp;a\\ge0<br>\\\\ -1 &amp;a&lt;0<br>\\end{cases} = 2u(a)-1<br>$$</p>\n<ol start=\"4\">\n<li>Sigmoid:</li>\n</ol>\n<p>$$<br>\\phi(a) = \\sigma(a) = \\frac{1}{1+e^{-a}} \\\\<br>\\frac{d\\phi}{da} = \\phi(a)[1-\\phi(a)]<br>$$</p>\n<ol start=\"5\">\n<li>Hyperbolic tangent:</li>\n</ol>\n<p>$$<br>\\phi(a) = tanh(x) = \\frac{e^a-e^{-a}}{e^a+e^{-a}} = 2\\sigma(a)-1\\\\<br>$$</p>\n<ol start=\"6\">\n<li>Rectifier linear unit (ReLU):</li>\n</ol>\n<p>$$<br>\\phi(a) = ReLU(a) = max(a,0) = \\begin{cases}<br>     a &amp;a\\ge0<br>\\\\ 0 &amp;a&lt;0<br>\\end{cases}<br>$$</p>\n<ol start=\"7\">\n<li>Softplus (a smooth approximation to ReLU):</li>\n</ol>\n<p>$$<br>\\phi(a) = ln(1+e^a)<br>$$</p>\n<ol start=\"8\">\n<li>Leaky ReLU (non-zero gradient for a&lt;0):</li>\n</ol>\n<p>$$<br>\\phi(a) = \\begin{cases}<br>     a &amp;a\\ge0<br>\\\\ 0.001a &amp;a&lt;0<br>\\end{cases}<br>$$</p>\n<ol start=\"9\">\n<li>Exponential linear unit (ELU) (non-zero gradient for a&lt;0):</li>\n</ol>\n<p>$$<br>\\phi(a) = \\begin{cases}<br>     a &amp;a\\ge0<br>\\\\ a(e^a-1) &amp;a&lt;0<br>\\end{cases}<br>$$</p>\n<ol start=\"10\">\n<li>Softmax:</li>\n</ol>\n<p>$$<br>\\phi_i(a) = \\frac{e^{a_i}}{\\sum_j e^{a_j}}, \\quad 1 \\le j \\le c \\\\<br>\\frac{\\partial\\phi_i(a)}{\\partial a_j} = \\begin{cases}<br>     \\phi_i(a)[1-\\phi_i(a)] &amp;i = j<br>\\\\ -\\phi_i(a)\\phi_j(a) &amp;i \\ne j<br>\\end{cases}<br>$$</p>\n<h2 id=\"Universal-Approximation-Theorem\"><a href=\"#Universal-Approximation-Theorem\" class=\"headerlink\" title=\"Universal Approximation Theorem\"></a>Universal Approximation Theorem</h2><p>The universal approximation theorem states that a feed-forward neural network with a linear output layer (φ L (a) = a) and</p>\n<ul>\n<li><p>at least one hidden layer with</p>\n</li>\n<li><p>a nonlinear activation function</p>\n</li>\n</ul>\n<p>can approximate any continuous (nonlinear) function y (on compact input sets) to arbitrary accuracy.</p>\n<p>Comments:</p>\n<ul>\n<li>arbitrary accuracy: with an increasing number of hidden neurons.</li>\n<li>valid for a wide range of nonlinear activation functions, but excluding polynomials.</li>\n<li>minimum requirement for universal approximation: $W_2\\phi_1(W_1x_0+b_1)+b_2$</li>\n</ul>\n<h2 id=\"Loss\"><a href=\"#Loss\" class=\"headerlink\" title=\"Loss\"></a>Loss</h2><p>L2 loss (assuming true distribution y is estimation + white Gaussian noise):<br>$$<br>L(\\theta)=\\sum_{n=1}^N||y(n)-f(x(n);\\theta)||^2<br>$$<br>L1 loss (assuming noise follows Laplace distribution):<br>$$<br>L(\\theta)=\\sum_{n=1}^N|y(n)-f(x(n);\\theta)|<br>$$<br>Categorical cross entropy for classification:<br>$$<br>L(\\theta)=\\sum_{n=1}^N[-y^T(n)ln(f(x(n);\\theta))]<br>$$</p>\n<h2 id=\"Back-Propagation\"><a href=\"#Back-Propagation\" class=\"headerlink\" title=\"Back Propagation\"></a>Back Propagation</h2><p><img src=\"https://github.com/dbddqy/Note/raw/master/Deep_Learning/pics/BP_0.jpg\" alt=\"\"></p>\n<p><img src=\"https://github.com/dbddqy/Note/raw/master/Deep_Learning/pics/BP_1.jpg\" alt=\"\"></p>\n<h2 id=\"Epoch-and-Mini-batch\"><a href=\"#Epoch-and-Mini-batch\" class=\"headerlink\" title=\"Epoch and Mini-batch\"></a>Epoch and Mini-batch</h2><p><img src=\"https://github.com/dbddqy/Note/raw/master/Deep_Learning/pics/minibatch.png\" alt=\"\"></p>\n","categories":["theory"],"tags":["deep learning"]},{"title":"[Theory] [Deep Learning] ch3","url":"https://dbddqy.github.io/2019/05/03/t-deep_learning_ch3/","content":"<h1 id=\"Machine-Learning-Basics\"><a href=\"#Machine-Learning-Basics\" class=\"headerlink\" title=\"Machine Learning Basics\"></a>Machine Learning Basics</h1><h2 id=\"Estimate-of-PDF-and-CDF-from-data\"><a href=\"#Estimate-of-PDF-and-CDF-from-data\" class=\"headerlink\" title=\"Estimate of PDF and CDF from data\"></a>Estimate of PDF and CDF from data</h2><p>x(n), n=1,2,…,N are i.i.d. samples drawn from $p(x) \\thicksim N(0, 1)$. The empirical PDF and CDF can be calculated as:<br>$$<br>\\hat{p}(x)=\\frac{1}{N}\\sum_{x=1}^N \\delta(x-x(n))<br>\\\\ \\hat{F}(x)=\\frac{1}{N}\\sum_{x=1}^N u(x-x(n))<br>$$<br>u(x) is the unit step function and $\\delta$(x) is Dirac function.</p>\n<p><img src=\"https://github.com/dbddqy/Note/raw/master/Deep_Learning/pics/pdf_cdf_from_data.png\" alt=\"\"></p>\n<h2 id=\"Kullback–Leibler-Divergence\"><a href=\"#Kullback–Leibler-Divergence\" class=\"headerlink\" title=\"Kullback–Leibler Divergence\"></a>Kullback–Leibler Divergence</h2><p>Let p(x) and q(x) be two distribution of random vector <strong>x</strong><br>$$<br>D_{KL}(p||q) = \\int p(x)ln(\\frac{p(x)}{q(x)})dx = E_{x\\thicksim p}\\frac{p(x)}{q(x)} \\overset{disc}{=} \\sum_{i=1}^cln(\\frac{p(x)}{q(x)})<br>$$<br>Properties:</p>\n<ol>\n<li>non-negative</li>\n<li>equality $D_{KL}(p||q)=0 \\Leftrightarrow p(x)=q(x)$</li>\n<li>asymmetry $D_{KL}(p||q) \\ne D_{KL}(p||q)$</li>\n</ol>\n<p>Minimizing D_KL is equivalent to minimizing <strong>Cross Entropy</strong><br>$$<br>H(p,q)=-\\int_{-\\infty}^{\\infty}p(x)ln(q(x))dx<br>$$</p>\n","categories":["theory"],"tags":["deep learning"]},{"title":"[Project] [Software] Animation","url":"https://dbddqy.github.io/2019/04/02/p-animation/","content":"<h1 id=\"Animation\"><a href=\"#Animation\" class=\"headerlink\" title=\"Animation\"></a>Animation</h1><p><strong>(A collaboration with Ruqing Zhong)</strong></p>\n<p><a href=\"https://github.com/dbddqy/Animation\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/Github-Source%20Code-blue\" alt=\"\"></a> <a href=\"https://www.food4rhino.com/app/animation\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/food4rhino-v1.2-lightgrey\" alt=\"\"></a></p>\n<p>A lightweight Grasshopper plug-in helps designers to make better animations with less effort.</p>\n<p>Components:</p>\n<p><img src=\"https://github.com/dbddqy/Animation/blob/master/pics/0.jpg?raw=true\" alt=\"\"></p>\n<p>What it can achieve:</p>\n<ul>\n<li>Control viewport. By giving a camera position and a target point, you can easily control the viewport thus moving the camera, or even zoom in zoom out in your video/gif.</li>\n<li>Assign material. Material can be assigned to you objects, so you can have geometries changing color, reflectivity, transparency, etc. in your video/gif.</li>\n<li>Better visualization. This plug-in automatically bakes geometries into rhino environment before saving every frame (and automatically deletes them), so it can achieve all the Rhino visualization effects (proper shading, proper lighting and shadows, clipping planes, etc.) by adjusting your rhino display setting.</li>\n</ul>\n<p>3 example files:</p>\n<ol>\n<li><p>A growing box, changing color, with the camera moving around.  </p>\n<p><img src=\"https://github.com/dbddqy/Animation/raw/master/pics/1.gif\" alt=\"\"></p>\n</li>\n<li><p>Small emerging pieces, with the camera moving around.</p>\n<p><img src=\"https://github.com/dbddqy/Animation/raw/master/pics/2.gif\" alt=\"\"></p>\n</li>\n<li><p>Falling balls, changing color, done together with Kangaroo Physics.</p>\n<p><img src=\"https://github.com/dbddqy/Animation/raw/master/pics/3.gif\" alt=\"\"></p>\n</li>\n</ol>\n","categories":["project"],"tags":["grasshopper"]},{"title":"[Project] [Software] Mahjong Scorer","url":"https://dbddqy.github.io/2018/11/28/p-mahjong_scorer/","content":"<h1 id=\"Mahjong-Scorer\"><a href=\"#Mahjong-Scorer\" class=\"headerlink\" title=\"Mahjong Scorer\"></a>Mahjong Scorer</h1><p><a href=\"https://github.com/dbddqy/mahjong_scorer\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/Github-Source%20Code-blue\" alt=\"\"></a></p>\n<p>国标麻将计分器(iOS)</p>\n<p>(目前只支持中文，后续将支持英文)</p>\n<p>国标麻将番种众多。 对于初学者来说，最大的困难之一是识别番种与计算番数。 此iOS应用程序可帮助您进行所有计算。您只需指定获胜时的14张牌。 它可以被用来在实战中计算番种，或是学习国标麻将的番种规则。</p>\n<p>支持所有81个番种。</p>\n<hr>\n<p>An iOS app for calculating mahjong scores (<a href=\"https://en.wikipedia.org/wiki/Mahjong_Competition_Rules\" target=\"_blank\" rel=\"noopener\">Chinese official competition rules</a>).</p>\n<p>(Currently only in chinese, english version will be added in the future.)</p>\n<p>The Mahjong Competition Rule, is characterised by dozens of possible combinations and a rather complex and inventive set of patterns. One of the majoy difficulties for beginners is to regconize patterns and calculate the corresponding scores. This iOS app helps to do all the calculation and you just need to specify the 14 tiles when you win. It can be used in games, as well as learning the competitional rules. </p>\n<p>All 81 patterns are supported.</p>\n<hr>\n<p>An example of Mixed Shifted Chows (三色三步高):</p>\n<p><img src=\"https://github.com/dbddqy/mahjong_scorer/blob/master/pics_md/Mixed_Shifted_Chows.gif?raw=true\" alt=\"\"></p>\n<p>An example of Full Flush (清一色) and Three Concealed Pung (三暗刻):</p>\n<p><img src=\"https://github.com/dbddqy/mahjong_scorer/blob/master/pics_md/Full_Flush.gif?raw=true\" alt=\"\"></p>\n<p>An example of Thirteen Orphans (十三幺):</p>\n<p><img src=\"https://github.com/dbddqy/mahjong_scorer/blob/master/pics_md/Thirteen_Orphans.gif?raw=true\" alt=\"\"></p>\n<hr>\n<p>Video of the full demo:</p>\n<div align=\"left\">\n    <iframe src=\"https://player.vimeo.com/video/442052152\" width=\"540\" height=\"1080\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen></iframe>\n    <p><a href=\"https://vimeo.com/442052152\" target=\"_blank\" rel=\"noopener\">Mahjong Scorer</a></p>\n</div>","categories":["project"],"tags":["iOS"]},{"title":"[Project] Design Projects 2014-17","url":"https://dbddqy.github.io/2017/11/05/p-design_projects_14_17/","content":"<h1 id=\"Design-Projects-2014-17\"><a href=\"#Design-Projects-2014-17\" class=\"headerlink\" title=\"Design Projects 2014-17\"></a>Design Projects 2014-17</h1><p><a href=\"https://issuu.com/yue_qi/docs/design_projects_2014-2017\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/Issuu-Pub-orange\" alt=\"\"></a></p>\n<p>View full version on <a href=\"https://issuu.com/yue_qi/docs/design_projects_2014-2017\" target=\"_blank\" rel=\"noopener\">Issuu</a>.</p>\n<h2 id=\"Social-Center-in-Genoa\"><a href=\"#Social-Center-in-Genoa\" class=\"headerlink\" title=\"Social Center in Genoa\"></a>Social Center in Genoa</h2><p><img src=\"/pics/p-design_projects_14_17/p1_0.png\" alt=\"\"></p>\n<p><img src=\"/pics/p-design_projects_14_17/p1_1.png\" alt=\"\"></p>\n<p><img src=\"/pics/p-design_projects_14_17/p1_2.png\" alt=\"\"></p>\n<h2 id=\"Animal-Shelter\"><a href=\"#Animal-Shelter\" class=\"headerlink\" title=\"Animal Shelter\"></a>Animal Shelter</h2><p><img src=\"/pics/p-design_projects_14_17/p2_0.png\" alt=\"\"></p>\n<p><img src=\"/pics/p-design_projects_14_17/p2_2.png\" alt=\"\"></p>\n<p><img src=\"/pics/p-design_projects_14_17/p2_1.png\" alt=\"\"></p>\n<h2 id=\"Weaving\"><a href=\"#Weaving\" class=\"headerlink\" title=\"Weaving\"></a>Weaving</h2><p><img src=\"/pics/p-design_projects_14_17/p3_0.png\" alt=\"\"></p>\n<h2 id=\"Tensegrity\"><a href=\"#Tensegrity\" class=\"headerlink\" title=\"Tensegrity\"></a>Tensegrity</h2><p><img src=\"/pics/p-design_projects_14_17/p4_0.png\" alt=\"\"></p>\n<p><img src=\"/pics/p-design_projects_14_17/p4_1.png\" alt=\"\"></p>\n","categories":["project"],"tags":[]},{"title":"about","url":"https://dbddqy.github.io/about/index.html","content":"","categories":[],"tags":[]},{"title":"category","url":"https://dbddqy.github.io/category/index.html","content":"","categories":[],"tags":[]},{"title":"link","url":"https://dbddqy.github.io/link/index.html","content":"","categories":[],"tags":[]},{"title":"project","url":"https://dbddqy.github.io/project/index.html","content":"","categories":[],"tags":[]},{"title":"search","url":"https://dbddqy.github.io/search/index.html","content":"","categories":[],"tags":[]},{"title":"tag","url":"https://dbddqy.github.io/tag/index.html","content":"","categories":[],"tags":[]}]