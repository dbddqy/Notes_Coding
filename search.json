[{"title":"[Publication] Working with uncertainties, an adaptive fabrication workflow for bamboo structures","url":"https://dbddqy.github.io/2020/08/02/pub-cdrf2020/","content":"<h1 id=\"Working-with-uncertainties-an-adaptive-fabrication-workflow-for-bamboo-structures\"><a href=\"#Working-with-uncertainties-an-adaptive-fabrication-workflow-for-bamboo-structures\" class=\"headerlink\" title=\"Working with uncertainties, an adaptive fabrication workflow for bamboo structures\"></a>Working with uncertainties, an adaptive fabrication workflow for bamboo structures</h1><p><strong>(A collaboration with Ruqing Zhong, Benjamin Kaiser, Long Nguyen, Hans Jakob Wagner, Alexander Verl and Achim Menges)</strong></p>\n<p>This research in presented on the <a href=\"https://www.digitalfutures.world/cdrf2020\" target=\"_blank\" rel=\"noopener\">CERF 2020 Conference</a> and will be soon published by Spinger.</p>\n","categories":["publication"],"tags":[]},{"title":"[Theory] [Probabilistic Robotics] ch2","url":"https://dbddqy.github.io/2020/07/23/t-probabilistic_robotics_ch2/","content":"<h1 id=\"Recursive-State-Estimation\"><a href=\"#Recursive-State-Estimation\" class=\"headerlink\" title=\"Recursive State Estimation\"></a>Recursive State Estimation</h1><h2 id=\"Conditional-Independence\"><a href=\"#Conditional-Independence\" class=\"headerlink\" title=\"Conditional Independence\"></a>Conditional Independence</h2><p><em>x</em> and <em>y</em> are independent under the condition <em>z</em>:<br>$$<br>p(x,y|z) = p(x|z)p(y|z) \\tag{2.17}<br>$$</p>\n<p>which is equivalent to:<br>$$<br>\\begin{cases}<br>     p(x|z) = p(x|z,y)<br>\\\\ p(y|z) = p(y|z,x) \\tag{2.18-19}<br>\\end{cases}<br>$$<br><strong>Proof</strong>:</p>\n<p>WLOG, insert (2.18) into (2.17) right side, get<br>$$<br>p(x|z,y)p(y|z) = \\frac{p(x|z,y)p(y,z)}{p(z)} = \\frac{p(x,y,z)}{p(z)} = p(x,y|z)<br>$$<br>however x and y are not necessarily independent, see book (2.20-21)</p>\n<h2 id=\"Probabilistic-Generative-Laws\"><a href=\"#Probabilistic-Generative-Laws\" class=\"headerlink\" title=\"Probabilistic Generative Laws\"></a>Probabilistic Generative Laws</h2><p><strong>Robot States</strong>: x<sub>0</sub>, x<sub>1</sub>, … , x<sub>t</sub></p>\n<p><strong>Environment Measurement</strong>: z<sub>1</sub>, … , z<sub>t</sub></p>\n<p><strong>Control data</strong>: u<sub>1</sub>, … , u<sub>t</sub></p>\n<p>(assume take action u first, than take measurement z)</p>\n<p>If x is <strong>complete</strong>, then x<sub>t</sub> only depends on x<sub>t-1</sub> and u<sub>t</sub>:<br>$$<br>p(x_t|x_{0:t-1},z_{1:t-1},u_{1:t}) = p(x_t|x_{t-1}, u_t) \\tag{2.31}<br>$$<br>and z<sub>t</sub> depends only on x<sub>t</sub>:<br>$$<br>p(z_t|x_{0:t},z_{1:t-1},u_{1:t}) = p(z_t|x_t) \\tag{2.32}<br>$$<br>(2.31) is called <strong>state transition probability</strong> and (2.32) is called <strong>measurement probability</strong>. They together form the <strong>Hidden Markov Model</strong>.</p>\n<p><img src=\"https://github.com/dbddqy/Note/blob/master/Probabilistic_Robotics/pics/HMM.png?raw=true\" alt=\"\"></p>\n<h2 id=\"Belief-Distributions\"><a href=\"#Belief-Distributions\" class=\"headerlink\" title=\"Belief Distributions\"></a>Belief Distributions</h2><p><strong>Belief</strong>: with all the measurements and control data, the distribution of state <em>x</em>, where the robot believes it is.</p>\n<p>Before measurement:<br>$$<br>\\overline{bel}(x_t) = p(x_t | z_{1:t-1}, u_{1:t}) \\tag{2.34}<br>$$<br>After measurement:<br>$$<br>bel(x_t) = p(x_t | z_{1:t}, u_{1:t}) \\tag{2.33}<br>$$</p>\n<h2 id=\"Bayes-Filters\"><a href=\"#Bayes-Filters\" class=\"headerlink\" title=\"Bayes Filters\"></a>Bayes Filters</h2><p><strong>Prediction</strong>:<br>$$<br>\\overline{bel}(x_i) = \\int{p(x_t | u_t, x_{t-1})bel(x_{t-1})}dx_{t-1}<br>$$<br><strong>Correction</strong> (measurement update):<br>$$<br>bel(x_i) = \\eta{p(z_t | x_t)\\overline{bel}(x_t)}<br>$$</p>\n","categories":["theory"],"tags":["robotics","probabilistic robotics"]},{"title":"[Project] [Software] Visual Kinematics","url":"https://dbddqy.github.io/2020/07/17/p-visual_kinematics/","content":"<h1 id=\"Visual-Kinematics\"><a href=\"#Visual-Kinematics\" class=\"headerlink\" title=\"Visual Kinematics\"></a>Visual Kinematics</h1><p><a href=\"https://github.com/dbddqy/visual_kinematics\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/Github-Source%20Code-blue\" alt=\"\"></a> <a href=\"https://pypi.org/project/visual-kinematics/\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/PyPI-0.0.7-orange\" alt=\"\"></a></p>\n<p>This project concentrates on an python-based robotics toolbox for <strong>kinematics calculation</strong>, <strong>robot pose visualization</strong> and <strong>robot path planning</strong>.</p>\n<h2 id=\"Requirements\"><a href=\"#Requirements\" class=\"headerlink\" title=\"Requirements\"></a>Requirements</h2><ul>\n<li>easy to use, only need DH parameters as input (so numerical inverse required).</li>\n<li>support both classic and modified DH parameters</li>\n<li>support robot with different degrees of freedom</li>\n<li>support different representations for rotaion (euler angles, angle axis, quaternion, etc.)</li>\n<li>support high performance analytical inverse (has to be user defined)</li>\n<li>visualizing robot pose and trajectory (a slider for simulating the trajectory)</li>\n<li>support path interpolation in joint space and cartesian space (p2p and linear) </li>\n</ul>\n<h2 id=\"Some-Results\"><a href=\"#Some-Results\" class=\"headerlink\" title=\"Some Results\"></a>Some Results</h2><p><strong>Trajectory:</strong></p>\n<p><img src=\"https://github.com/dbddqy/visual_kinematics/raw/master/pics/trajectory.gif?raw=true\" alt=\"\"></p>\n<p><strong>Linear motion with analytical inverse:</strong></p>\n<p><img src=\"https://github.com/dbddqy/visual_kinematics/raw/master/pics/analytical_inv.gif?raw=true\" alt=\"\"></p>\n<p><strong>7-axis:</strong></p>\n<p><img src=\"https://github.com/dbddqy/visual_kinematics/raw/master/pics/7-axis.gif?raw=true\" alt=\"\"></p>\n","categories":["project"],"tags":["robotics"]},{"title":"[Project] Platform-based Robotic Timber Architecture","url":"https://dbddqy.github.io/2020/06/25/p-digital_future_vision/","content":"<h1 id=\"Platform-based-Robotic-Timber-Architecture\"><a href=\"#Platform-based-Robotic-Timber-Architecture\" class=\"headerlink\" title=\"Platform-based Robotic Timber Architecture\"></a>Platform-based Robotic Timber Architecture</h1><p><strong>This is a Digital Future 2020 workshop given by CAUP (College of Architecture and Urban Planning), Tongji University and ICD (Institute for Computational Design and Construction) University of Stuttgart collaboratively. I was involved as a student research assistant from ICD side and developed the fiducial marker based platform localization strategy.</strong></p>\n<p>Check the project info <a href=\"https://www.digitalfutures.world/workshops-asia-pacific-blog/yuan-menges\" target=\"_blank\" rel=\"noopener\">here</a></p>\n","categories":["project"],"tags":["robotics"]},{"title":"[Project] Robotic Craftsmanship","url":"https://dbddqy.github.io/2020/02/06/p-robotic_craftsmanship/","content":"<h1 id=\"Robotic-Craftsmanship\"><a href=\"#Robotic-Craftsmanship\" class=\"headerlink\" title=\"Robotic Craftsmanship\"></a>Robotic Craftsmanship</h1><p><strong>(A collaboration with Yanan Guo and Ruqing Zhong)</strong></p>\n<p><a href=\"https://github.com/dbddqy/machine_perception/tree/master/demo2.2_detect_cylinder\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/Github-Repo-blue\" alt=\"\"></a></p>\n<p><img src=\"/pics/p-robotic_craftsmanship/result.png\" alt=\"\"></p>\n<p>This is a final project of 2020 behavior fabrication seminar at University of Stuttgart.</p>\n<p>Craftman can intuitively handle material without knowing its precise dimensions in advance. However, robots are still in most of the cases blind and repeating the same motion over and over again.</p>\n<p><img src=\"/pics/p-robotic_craftsmanship/material_uncertainty.png\" alt=\"\"></p>\n<p>This project explores the posibility of using robotic arm to cut the mortise of bamboo connections. As the exact geometry of the bamboo is unknown, the robot should be equipped with some sensor to perceive the material.</p>\n<p><strong>Sensor:</strong></p>\n<ul>\n<li>RGBD camera: Realsense D415</li>\n</ul>\n<p><strong>Algorithms used:</strong></p>\n<ul>\n<li>region growing for segmentation (initial point given by aruco markers)<br>(that is a short cut we take, a more proper and general method can be deep neural network based instance segmentaion, e.g., Mask R-CNN)</li>\n<li>ICP (iterative closest point) for aligning mutiple frames</li>\n<li>least squares cylinder fitting for estimating cylinder parameters</li>\n</ul>\n<p><strong>Segmentation:</strong></p>\n<p><img src=\"/pics/p-robotic_craftsmanship/segmentation.gif\" alt=\"\"></p>\n<p><strong>Robotic milling:</strong></p>\n<p><img src=\"/pics/p-robotic_craftsmanship/milling.gif\" alt=\"\"></p>\n<p><strong>Result:</strong></p>\n<p><img src=\"/pics/p-robotic_craftsmanship/result.gif\" alt=\"\"></p>\n","categories":["project"],"tags":["ITECH master"]},{"title":"[Project] Sechs (ongoing)","url":"https://dbddqy.github.io/2019/10/01/p-sechs/","content":"<h1 id=\"Sechs\"><a href=\"#Sechs\" class=\"headerlink\" title=\"Sechs\"></a>Sechs</h1><p><strong>(A collaboration with An Mo)</strong></p>\n<p><a href=\"https://github.com/moanan/sechs\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/Github-Repo-blue\" alt=\"\"></a></p>\n<p><img src=\"/pics/p-sechs/A1-A5_20200706.jpg\" alt=\"\"></p>\n<p><strong>Towards a high performance force-aware 6-axis robot!</strong></p>\n<hr>\n<h2 id=\"Design-Requirements\"><a href=\"#Design-Requirements\" class=\"headerlink\" title=\"Design Requirements\"></a>Design Requirements</h2><ul>\n<li>6 axis</li>\n<li>High performance<ul>\n<li>Joint acceleration: 2π [rad/s] (except J1: 1π [rad/s])</li>\n<li>Tool payload: 2 [kg]</li>\n</ul>\n</li>\n<li>Robot teaching by hand (force-aware)</li>\n<li>Desktop size</li>\n<li>3d printing for most parts</li>\n</ul>\n<hr>\n<h2 id=\"DH-Parameters\"><a href=\"#DH-Parameters\" class=\"headerlink\" title=\"DH Parameters\"></a>DH Parameters</h2><p>The parameters listed here are not final.</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Axis</th>\n<th align=\"left\">d</th>\n<th align=\"left\">theta</th>\n<th align=\"left\">a</th>\n<th align=\"left\">alpha</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">1</td>\n<td align=\"left\">0.112</td>\n<td align=\"left\">0.0</td>\n<td align=\"left\">0.0</td>\n<td align=\"left\">0.0</td>\n</tr>\n<tr>\n<td align=\"left\">2</td>\n<td align=\"left\">0.0</td>\n<td align=\"left\">-0.5 * pi</td>\n<td align=\"left\">0.0</td>\n<td align=\"left\">-0.5 * pi</td>\n</tr>\n<tr>\n<td align=\"left\">3</td>\n<td align=\"left\">-0.0525</td>\n<td align=\"left\">0.5 * pi</td>\n<td align=\"left\">0.2</td>\n<td align=\"left\">0.0</td>\n</tr>\n<tr>\n<td align=\"left\">4</td>\n<td align=\"left\">0.2</td>\n<td align=\"left\">0.0</td>\n<td align=\"left\">0.0</td>\n<td align=\"left\">0.5 * pi</td>\n</tr>\n<tr>\n<td align=\"left\">5</td>\n<td align=\"left\">0.085</td>\n<td align=\"left\">0.0</td>\n<td align=\"left\">0.0</td>\n<td align=\"left\">-0.5 * pi</td>\n</tr>\n<tr>\n<td align=\"left\">6</td>\n<td align=\"left\">0.08</td>\n<td align=\"left\">0.0</td>\n<td align=\"left\">0.0</td>\n<td align=\"left\">0.5 * pi</td>\n</tr>\n</tbody></table>\n<hr>\n<h2 id=\"Load-Estimation\"><a href=\"#Load-Estimation\" class=\"headerlink\" title=\"Load Estimation\"></a>Load Estimation</h2><p>The python-based <a href=\"https://github.com/moanan/sechs/tree/master/1-calculation/torqueLoadCase\" target=\"_blank\" rel=\"noopener\">Dynamic Model</a> is built for estimating required torque for each joint.</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Axis</th>\n<th align=\"left\">Required Torque[Nm]</th>\n<th align=\"left\">Motor</th>\n<th align=\"left\">Motor Torque[Nm]</th>\n<th align=\"left\">Reduction</th>\n<th align=\"left\">Output Torque[Nm]</th>\n<th align=\"left\">Safety Factor</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">1</td>\n<td align=\"left\">3.4</td>\n<td align=\"left\">Sunnysky X4110S 170kv</td>\n<td align=\"left\">~2</td>\n<td align=\"left\">4:1 (belt)</td>\n<td align=\"left\">8</td>\n<td align=\"left\">2.3</td>\n</tr>\n<tr>\n<td align=\"left\">2</td>\n<td align=\"left\">28.8</td>\n<td align=\"left\">SK3 - 4250-350kv</td>\n<td align=\"left\">1.18</td>\n<td align=\"left\">30:1</td>\n<td align=\"left\">35.4</td>\n<td align=\"left\">1.2</td>\n</tr>\n<tr>\n<td align=\"left\">3</td>\n<td align=\"left\">11.7</td>\n<td align=\"left\">SK3 - 4250-350kv</td>\n<td align=\"left\">1.18</td>\n<td align=\"left\">15:1</td>\n<td align=\"left\">17.7</td>\n<td align=\"left\">1.5</td>\n</tr>\n<tr>\n<td align=\"left\">4</td>\n<td align=\"left\">2.6</td>\n<td align=\"left\">T-Motor AS 2820 880kv</td>\n<td align=\"left\">0.3</td>\n<td align=\"left\">19.2:1</td>\n<td align=\"left\">5.76</td>\n<td align=\"left\">2.2</td>\n</tr>\n<tr>\n<td align=\"left\">5</td>\n<td align=\"left\">3.9</td>\n<td align=\"left\">T-Motor AS 2820 880kv</td>\n<td align=\"left\">0.3</td>\n<td align=\"left\">19.2:1</td>\n<td align=\"left\">5.76</td>\n<td align=\"left\">1.5</td>\n</tr>\n<tr>\n<td align=\"left\">6</td>\n<td align=\"left\">N/A</td>\n<td align=\"left\">T-Motor AS 2820 880kv</td>\n<td align=\"left\">0.3</td>\n<td align=\"left\">19.2:1</td>\n<td align=\"left\">5.76</td>\n<td align=\"left\">N/A</td>\n</tr>\n</tbody></table>\n<hr>\n<h2 id=\"Mechanical-Design-20200706\"><a href=\"#Mechanical-Design-20200706\" class=\"headerlink\" title=\"Mechanical Design 20200706\"></a>Mechanical Design 20200706</h2><p><img src=\"/pics/p-sechs/mechanical_design_20200706.png\" alt=\"\"></p>\n<hr>\n<h2 id=\"Testing-20200716\"><a href=\"#Testing-20200716\" class=\"headerlink\" title=\"Testing 20200716\"></a>Testing 20200716</h2><div align=\"left\">\n    <iframe src=\"https://player.vimeo.com/video/443796757\" width=\"320\" height=\"640\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen></iframe>\n    <p><a href=\"https://vimeo.com/443796757\" target=\"_blank\" rel=\"noopener\">Testing Jogging</a></p>\n</div>\n\n<div align=\"left\">\n    <iframe src=\"https://player.vimeo.com/video/443797208\" width=\"320\" height=\"640\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen></iframe>\n    <p><a href=\"https://vimeo.com/443797208\" target=\"_blank\" rel=\"noopener\">Testing Teaching</a></p>\n</div>\n\n<div align=\"left\">\n    <iframe src=\"https://player.vimeo.com/video/443797861\" width=\"320\" height=\"640\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen></iframe>\n    <p><a href=\"https://vimeo.com/443797861\" target=\"_blank\" rel=\"noopener\">Testing Offline Planning</a></p>\n</div>\n","categories":["project"],"tags":["robotics","ongoing"]},{"title":"[Project] Move-X (ongoing)","url":"https://dbddqy.github.io/2019/07/29/p-move_x/","content":"<h1 id=\"Move-X\"><a href=\"#Move-X\" class=\"headerlink\" title=\"Move-X\"></a>Move-X</h1><p><a href=\"https://github.com/dbddqy/mobile_robot\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/Github-Repo-blue\" alt=\"\"></a></p>\n<p><strong>By building a omni-directional robot platform, this project is mainly for me to learn mechanics and control, as well as robot state estimation and sensor fusion for autonomous vehicles.</strong></p>\n<h2 id=\"Low-Level\"><a href=\"#Low-Level\" class=\"headerlink\" title=\"Low Level:\"></a>Low Level:</h2><p>Mecanum wheels locomotion strategy. Using DJI Robomaster hardware.</p>\n<ul>\n<li>mechanical design</li>\n<li>mecanum wheel kinematics</li>\n<li>motor control: closed-loop velocity control</li>\n<li>publish wheel odometry and IMU data through serial communication</li>\n</ul>\n<h2 id=\"High-Level\"><a href=\"#High-Level\" class=\"headerlink\" title=\"High Level:\"></a>High Level:</h2><p>Laser-based SLAM and Navigation.</p>\n<ul>\n<li>implement slam-gmapping</li>\n<li>implement navigation related ros packages</li>\n<li>develop own solution for slam and sensor fusion<br>(combining the theoretical study of <em>Probabilistic Robotics</em> and <em>State Estimation for Robotics</em>)</li>\n</ul>\n<h2 id=\"Testing-Chassis-20200709\"><a href=\"#Testing-Chassis-20200709\" class=\"headerlink\" title=\"Testing Chassis 20200709\"></a>Testing Chassis 20200709</h2><p><img src=\"https://github.com/dbddqy/mobile_robot/raw/master/pics/test_chassis.gif\" alt=\"\"></p>\n<h2 id=\"Testing-odometry-20200710\"><a href=\"#Testing-odometry-20200710\" class=\"headerlink\" title=\"Testing odometry 20200710\"></a>Testing odometry 20200710</h2><p><img src=\"https://github.com/dbddqy/mobile_robot/raw/master/pics/test_odom.gif\" alt=\"\"></p>\n<h2 id=\"Testing-slam-gmapping-20200722\"><a href=\"#Testing-slam-gmapping-20200722\" class=\"headerlink\" title=\"Testing slam-gmapping 20200722\"></a>Testing slam-gmapping 20200722</h2><p><img src=\"https://github.com/dbddqy/mobile_robot/raw/master/pics/test_gmapping.gif\" alt=\"\"></p>\n","categories":["project"],"tags":["robotics","ongoing"]},{"title":"[Project] Robotic Censorship","url":"https://dbddqy.github.io/2019/07/21/p-robotic_censorship/","content":"<h1 id=\"Robotic-Censorship\"><a href=\"#Robotic-Censorship\" class=\"headerlink\" title=\"Robotic Censorship\"></a>Robotic Censorship</h1><p><strong>(A collaboration with Zhetao Dong, Yanan Guo and Hooman Salyani)</strong></p>\n<p><a href=\"https://github.com/dbddqy/end_effector\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/Github-Repo-blue\" alt=\"\"></a></p>\n<p><img src=\"https://github.com/dbddqy/end_effector/blob/master/pics/process.gif?raw=true\" alt=\"\"></p>\n<p>This is a final project of 2019 CDDF seminar (computational design and digital fabrication) at University of Stuttgart.</p>\n<blockquote>\n<p>This project use use robot and object recognition algorithm to apply a block of paint for censorship.</p>\n</blockquote>\n<p>The robot has a stereo camera module equipped on its end-effector for object detection and depth inference, as well as two stepper motors and gear system for applying the paint.</p>\n<p>The robot goes to some pre-defined scanning locations and takes the scan. If any human face is found, it goes above that face and paint on it, otherwise it goes to next scan location. If there is little paint left in the syringe, it performs the auto-reload.</p>\n<p><strong>Hardware:</strong></p>\n<ul>\n<li>Raspberry Pi 3B+</li>\n<li>Stereo camera module</li>\n<li>Stepper Motors</li>\n<li>Gear and rack</li>\n<li>Syringe</li>\n</ul>\n<p><strong>Software:</strong></p>\n<ul>\n<li>OpenCV (python wrapper)</li>\n<li>Tensorflow (not used in the end)</li>\n</ul>\n<p><strong>Process:</strong></p>\n<p><img src=\"https://github.com/dbddqy/end_effector/blob/master/pics/process.png?raw=true\" alt=\"\"></p>\n<p><strong>End_effector:</strong></p>\n<p><img src=\"https://github.com/dbddqy/end_effector/blob/master/pics/end_effector.png?raw=true\" alt=\"\"></p>\n<p><strong>Communication with the KUKA robot:</strong></p>\n<p><img src=\"https://github.com/dbddqy/end_effector/blob/master/pics/communication.png?raw=true\" alt=\"\"></p>\n<p><strong>Full video:</strong></p>\n<div align=\"left\">\n    <iframe src=\"https://player.vimeo.com/video/443476046\" width=\"640\" height=\"360\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen></iframe>\n    <p><a href=\"https://vimeo.com/443476046\" target=\"_blank\" rel=\"noopener\">Robotic Sensorship 01</a></p>\n</div>\n\n<div align=\"left\">\n    <iframe src=\"https://player.vimeo.com/video/443476348\" width=\"640\" height=\"360\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen></iframe>\n    <p><a href=\"https://vimeo.com/443476348\" target=\"_blank\" rel=\"noopener\">Robotic Sensorship 02</a></p>\n</div>\n","categories":["project"],"tags":["ITECH master"]},{"title":"[Theory] [Deep Learning] ch3","url":"https://dbddqy.github.io/2019/05/03/t-deep_learning_ch3/","content":"<h1 id=\"Machine-Learning-Basics\"><a href=\"#Machine-Learning-Basics\" class=\"headerlink\" title=\"Machine Learning Basics\"></a>Machine Learning Basics</h1><h2 id=\"Estimate-of-PDF-and-CDF-from-data\"><a href=\"#Estimate-of-PDF-and-CDF-from-data\" class=\"headerlink\" title=\"Estimate of PDF and CDF from data\"></a>Estimate of PDF and CDF from data</h2><p>x(n), n=1,2,…,N are i.i.d. samples drawn from $p(x) \\thicksim N(0, 1)$. The empirical PDF and CDF can be calculated as:<br>$$<br>\\hat{p}(x)=\\frac{1}{N}\\sum_{x=1}^N \\delta(x-x(n))<br>\\\\ \\hat{F}(x)=\\frac{1}{N}\\sum_{x=1}^N u(x-x(n))<br>$$<br>u(x) is the unit step function and $\\delta$(x) is Dirac function.</p>\n<p><img src=\"pics/pdf_cdf_from_data.png\" alt=\"\"></p>\n<h2 id=\"Kullback–Leibler-Divergence\"><a href=\"#Kullback–Leibler-Divergence\" class=\"headerlink\" title=\"Kullback–Leibler Divergence\"></a>Kullback–Leibler Divergence</h2><p>Let p(x) and q(x) be two distribution of random vector <strong>x</strong><br>$$<br>D_{KL}(p||q) = \\int p(x)ln(\\frac{p(x)}{q(x)})dx = E_{x\\thicksim p}\\frac{p(x)}{q(x)} \\overset{disc}{=} \\sum_{i=1}^cln(\\frac{p(x)}{q(x)})<br>$$<br>Properties:</p>\n<ol>\n<li>non-negative</li>\n<li>equality $D_{KL}(p||q)=0 \\Leftrightarrow p(x)=q(x)$</li>\n<li>asymmetry $D_{KL}(p||q) \\ne D_{KL}(p||q)$</li>\n</ol>\n<p>Minimizing D_KL is equivalent to minimizing <strong>Cross Entropy</strong><br>$$<br>H(p,q)=-\\int_{-\\infty}^{\\infty}p(x)ln(q(x))dx<br>$$</p>\n","categories":["theory"],"tags":["deep learning"]},{"title":"[Project] [Software] Animation","url":"https://dbddqy.github.io/2019/04/02/p-animation/","content":"<h1 id=\"Animation\"><a href=\"#Animation\" class=\"headerlink\" title=\"Animation\"></a>Animation</h1><p><strong>(A collaboration with Ruqing Zhong)</strong></p>\n<p><a href=\"https://github.com/dbddqy/Animation\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/Github-Source%20Code-blue\" alt=\"\"></a> <a href=\"https://www.food4rhino.com/app/animation\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/food4rhino-v1.2-lightgrey\" alt=\"\"></a></p>\n<p>A lightweight Grasshopper plug-in helps designers to make better animations with less effort.</p>\n<p>Components:</p>\n<p><img src=\"https://github.com/dbddqy/Animation/blob/master/pics/0.jpg?raw=true\" alt=\"\"></p>\n<p>What it can achieve:</p>\n<ul>\n<li>Control viewport. By giving a camera position and a target point, you can easily control the viewport thus moving the camera, or even zoom in zoom out in your video/gif.</li>\n<li>Assign material. Material can be assigned to you objects, so you can have geometries changing color, reflectivity, transparency, etc. in your video/gif.</li>\n<li>Better visualization. This plug-in automatically bakes geometries into rhino environment before saving every frame (and automatically deletes them), so it can achieve all the Rhino visualization effects (proper shading, proper lighting and shadows, clipping planes, etc.) by adjusting your rhino display setting.</li>\n</ul>\n<p>3 example files:</p>\n<ol>\n<li><p>A growing box, changing color, with the camera moving around.  </p>\n<p><img src=\"https://github.com/dbddqy/Animation/raw/master/pics/1.gif\" alt=\"\"></p>\n</li>\n<li><p>Small emerging pieces, with the camera moving around.</p>\n<p><img src=\"https://github.com/dbddqy/Animation/raw/master/pics/2.gif\" alt=\"\"></p>\n</li>\n<li><p>Falling balls, changing color, done together with Kangaroo Physics.</p>\n<p><img src=\"https://github.com/dbddqy/Animation/raw/master/pics/3.gif\" alt=\"\"></p>\n</li>\n</ol>\n","categories":["project"],"tags":["grasshopper"]},{"title":"[Project] [Software] Mahjong Scorer","url":"https://dbddqy.github.io/2018/11/28/p-mahjong_scorer/","content":"<h1 id=\"Mahjong-Scorer\"><a href=\"#Mahjong-Scorer\" class=\"headerlink\" title=\"Mahjong Scorer\"></a>Mahjong Scorer</h1><p><a href=\"https://github.com/dbddqy/mahjong_scorer\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/Github-Source%20Code-blue\" alt=\"\"></a></p>\n<p>国标麻将计分器(iOS)</p>\n<p>(目前只支持中文，后续将支持英文)</p>\n<p>国标麻将番种众多。 对于初学者来说，最大的困难之一是识别番种与计算番数。 此iOS应用程序可帮助您进行所有计算。您只需指定获胜时的14张牌。 它可以被用来在实战中计算番种，或是学习国标麻将的番种规则。</p>\n<p>支持所有81个番种。</p>\n<hr>\n<p>An iOS app for calculating mahjong scores (<a href=\"https://en.wikipedia.org/wiki/Mahjong_Competition_Rules\" target=\"_blank\" rel=\"noopener\">Chinese official competition rules</a>).</p>\n<p>(Currently only in chinese, english version will be added in the future.)</p>\n<p>The Mahjong Competition Rule, is characterised by dozens of possible combinations and a rather complex and inventive set of patterns. One of the majoy difficulties for beginners is to regconize patterns and calculate the corresponding scores. This iOS app helps to do all the calculation and you just need to specify the 14 tiles when you win. It can be used in games, as well as learning the competitional rules. </p>\n<p>All 81 patterns are supported.</p>\n<hr>\n<p>An example of Mixed Shifted Chows (三色三步高):</p>\n<p><img src=\"https://github.com/dbddqy/mahjong_scorer/blob/master/pics_md/Mixed_Shifted_Chows.gif?raw=true\" alt=\"\"></p>\n<p>An example of Full Flush (清一色) and Three Concealed Pung (三暗刻):</p>\n<p><img src=\"https://github.com/dbddqy/mahjong_scorer/blob/master/pics_md/Full_Flush.gif?raw=true\" alt=\"\"></p>\n<p>An example of Thirteen Orphans (十三幺):</p>\n<p><img src=\"https://github.com/dbddqy/mahjong_scorer/blob/master/pics_md/Thirteen_Orphans.gif?raw=true\" alt=\"\"></p>\n<hr>\n<p>Video of the full demo:</p>\n<div align=\"left\">\n    <iframe src=\"https://player.vimeo.com/video/442052152\" width=\"540\" height=\"1080\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen></iframe>\n    <p><a href=\"https://vimeo.com/442052152\" target=\"_blank\" rel=\"noopener\">Mahjong Scorer</a></p>\n</div>","categories":["project"],"tags":["iOS"]},{"title":"[Project] Design Projects 2014-17","url":"https://dbddqy.github.io/2017/11/05/p-design_projects_14_17/","content":"<h1 id=\"Design-Projects-2014-17\"><a href=\"#Design-Projects-2014-17\" class=\"headerlink\" title=\"Design Projects 2014-17\"></a>Design Projects 2014-17</h1><p><a href=\"https://issuu.com/yue_qi/docs/design_projects_2014-2017\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/Issuu-Pub-orange\" alt=\"\"></a></p>\n<p>View full version on <a href=\"https://issuu.com/yue_qi/docs/design_projects_2014-2017\" target=\"_blank\" rel=\"noopener\">Issuu</a>.</p>\n<h2 id=\"Social-Center-in-Genoa\"><a href=\"#Social-Center-in-Genoa\" class=\"headerlink\" title=\"Social Center in Genoa\"></a>Social Center in Genoa</h2><p><img src=\"/pics/p-design_projects_14_17/p1_0.png\" alt=\"\"></p>\n<p><img src=\"/pics/p-design_projects_14_17/p1_1.png\" alt=\"\"></p>\n<p><img src=\"/pics/p-design_projects_14_17/p1_2.png\" alt=\"\"></p>\n<h2 id=\"Animal-Shelter\"><a href=\"#Animal-Shelter\" class=\"headerlink\" title=\"Animal Shelter\"></a>Animal Shelter</h2><p><img src=\"/pics/p-design_projects_14_17/p2_0.png\" alt=\"\"></p>\n<p><img src=\"/pics/p-design_projects_14_17/p2_2.png\" alt=\"\"></p>\n<p><img src=\"/pics/p-design_projects_14_17/p2_1.png\" alt=\"\"></p>\n<h2 id=\"Weaving\"><a href=\"#Weaving\" class=\"headerlink\" title=\"Weaving\"></a>Weaving</h2><p><img src=\"/pics/p-design_projects_14_17/p3_0.png\" alt=\"\"></p>\n<h2 id=\"Tensegrity\"><a href=\"#Tensegrity\" class=\"headerlink\" title=\"Tensegrity\"></a>Tensegrity</h2><p><img src=\"/pics/p-design_projects_14_17/p4_0.png\" alt=\"\"></p>\n<p><img src=\"/pics/p-design_projects_14_17/p4_1.png\" alt=\"\"></p>\n","categories":["project"],"tags":[]},{"title":"about","url":"https://dbddqy.github.io/about/index.html","content":"","categories":[],"tags":[]},{"title":"category","url":"https://dbddqy.github.io/category/index.html","content":"","categories":[],"tags":[]},{"title":"link","url":"https://dbddqy.github.io/link/index.html","content":"","categories":[],"tags":[]},{"title":"project","url":"https://dbddqy.github.io/project/index.html","content":"","categories":[],"tags":[]},{"title":"search","url":"https://dbddqy.github.io/search/index.html","content":"","categories":[],"tags":[]},{"title":"tag","url":"https://dbddqy.github.io/tag/index.html","content":"","categories":[],"tags":[]}]